{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3QTZmANKVTuk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QTZmANKVTuk",
    "outputId": "01439fbd-eb82-4160-f30c-c3e0fcd5fe4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/lib/python3.13/site-packages (2.3.5)\n",
      "Requirement already satisfied: datasets in /opt/homebrew/anaconda3/lib/python3.13/site-packages (4.5.0)\n",
      "Requirement already satisfied: torch in /opt/homebrew/anaconda3/lib/python3.13/site-packages (2.10.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (23.0.1)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (1.4.1)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy datasets torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ssVxV212Wqoj",
   "metadata": {
    "id": "ssVxV212Wqoj"
   },
   "source": [
    "### Import the Libraries\n",
    "\n",
    "The following Python modules will be used for our NanoGPT implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d549cc2c",
   "metadata": {
    "id": "d549cc2c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72b624",
   "metadata": {
    "id": "3a72b624"
   },
   "source": [
    "### Load the Dataset\n",
    "\n",
    "In the code block below, [Open-MELON dataset](https://huggingface.co/datasets/MartiHan/Open-MELON-VL-2.5K) is loaded using HuggingFace loaders and the figure captions are concatenated into one big training corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55323366",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55323366",
    "outputId": "12a8c2e8-1ffa-4d5a-f9a0-4c3bc14ea20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions: 2499\n",
      "Example caption: Small bowel polyps showing malignant cells located at the lamina propria and submucosa. The image is stained with hematoxylin and eosin (H&E) at × 4 magnification.\n"
     ]
    }
   ],
   "source": [
    "# ds_dict is a list of ['train', 'validation', 'test'] splits\n",
    "ds_dict = load_dataset(\"MartiHan/Open-MELON-VL-2.5K\")\n",
    "\n",
    "# in this example, all 3 splits are concatenated\n",
    "# validation set is later drawn from the text chunks of 'block_size'\n",
    "ds_all = concatenate_datasets(list(ds_dict.values()))\n",
    "\n",
    "captions = [str(x) for x in ds_all[\"caption\"]]\n",
    "print(\"Captions:\", len(captions))\n",
    "print(\"Example caption:\", captions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f1b8e",
   "metadata": {
    "id": "480f1b8e"
   },
   "source": [
    "#### Preprocess the Training Text\n",
    "\n",
    "We join separate captions with `<ENDC>` separator. This helps the model learn boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83044900",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83044900",
    "outputId": "ff5aeec9-92ca-4abb-81be-1e616e50c7bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text length (chars): 954168\n",
      "Small bowel polyps showing malignant cells located at the lamina propria and submucosa. The image is stained with hematoxylin and eosin (H&E) at × 4 magnification. Histopathology of small bowel polyps showing malignant cells arranged in diffuse sheets with the presence of intracytoplasmic and extracellular pigments. The image is stained with hematoxylin and eosin (H&E) at × 20 magnification. Small bowel polyps showing malignant cells located at the lamina propria and submucosa. The image is stained with hematoxylin and eosin (H&E) at × 4 magnification. Histopathology showing spindle cell uveal melanoma from a left eye choroidal pigmented malignant melanoma, classified as invasive in the ciliary body and cornea (stage IIIB, pT4bN0M0). The microscopic examination reveals a dense cell proliferation composed of small and medium fusiform (spindle) cells, along with evident pigment production. The tissue fragments were embedded in paraffin for this histopathological examination. Staining is \n"
     ]
    }
   ],
   "source": [
    "SEP = \"\\n<ENDC>\\n\"\n",
    "# SEP = \" \"\n",
    "text = SEP.join(captions)\n",
    "\n",
    "# Print the total number of characters in the dataset\n",
    "print(\"Training text length (chars):\", len(text))\n",
    "\n",
    "# Print the first 1000 characters of the constructed text corpus\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a82e7",
   "metadata": {
    "id": "f65a82e7"
   },
   "source": [
    "### Character-level Tokenizer\n",
    "\n",
    "We build a vocabulary of unique characters from the training text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b0f4cda",
   "metadata": {
    "id": "7b0f4cda"
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# mapping of characters to numerical tokens (by their order in vocabulary alphabet)\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "\n",
    "# mapping of numerical tokens back to characters\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "def encode(s: str):\n",
    "    \"\"\"\n",
    "    Converts character to a token.\n",
    "    Input: character (e.g. 'A')\n",
    "    Output: numerical token (e.g. 65)\n",
    "    \"\"\"\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ids):\n",
    "    \"\"\"\n",
    "    Converts token to a character.\n",
    "    Input: numerical token (e.g. 65)\n",
    "    Output: character (e.g. 'A')\n",
    "    \"\"\"\n",
    "    return \"\".join(itos[i] for i in ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8e373",
   "metadata": {
    "id": "27e8e373"
   },
   "source": [
    "Let's print some information about the vocabulary that we have created as well as some examples of encoding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7547580",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7547580",
    "outputId": "26bfd4bc-8742-4a16-fc3e-1d60707542ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 108\n",
      "Preview of the vocabulary: [' ', '\"', '#', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '§', '°', '±', '²', '´', 'µ', '×', 'í', 'α', 'μ', '\\u2005', '\\u2009', '‐', '–', '‘', '’', '“', '”', '→', '⇒', '≤', '≥', '⊷']\n",
      "\n",
      "--- Encoding Examples ---\n",
      "String:  male\n",
      "Tokens:  [70, 58, 69, 62]\n",
      "Mapping: 'm':70, 'a':58, 'l':69, 'e':62\n",
      "----------------------------------------\n",
      "String:  malignant\n",
      "Tokens:  [70, 58, 69, 66, 64, 71, 58, 71, 77]\n",
      "Mapping: 'm':70, 'a':58, 'l':69, 'i':66, 'g':64, 'n':71, 'a':58, 'n':71, 't':77\n",
      "----------------------------------------\n",
      "String:  melanoma\n",
      "Tokens:  [70, 62, 69, 58, 71, 72, 70, 58]\n",
      "Mapping: 'm':70, 'e':62, 'l':69, 'a':58, 'n':71, 'o':72, 'm':70, 'a':58\n",
      "----------------------------------------\n",
      "String:  malignant melanoma\n",
      "Tokens:  [70, 58, 69, 66, 64, 71, 58, 71, 77, 0, 70, 62, 69, 58, 71, 72, 70, 58]\n",
      "Mapping: 'm':70, 'a':58, 'l':69, 'i':66, 'g':64, 'n':71, 'a':58, 'n':71, 't':77, ' ':0, 'm':70, 'e':62, 'l':69, 'a':58, 'n':71, 'o':72, 'm':70, 'a':58\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of the vocabulary:\", vocab_size)\n",
    "print(\"Preview of the vocabulary:\", chars)\n",
    "\n",
    "examples = [\"male\", \"malignant\", \"melanoma\", \"malignant melanoma\"]\n",
    "\n",
    "print(\"\\n--- Encoding Examples ---\")\n",
    "\n",
    "for word in examples:\n",
    "    tokens = encode(word)\n",
    "\n",
    "    # Create a visual mapping of Char -> Token\n",
    "    mapping_str = \", \".join([f\"'{c}':{t}\" for c, t in zip(word, tokens)])\n",
    "\n",
    "    print(f\"String:  {word}\")\n",
    "    print(f\"Tokens:  {tokens}\")\n",
    "    print(f\"Mapping: {mapping_str}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa4771",
   "metadata": {
    "id": "69aa4771"
   },
   "source": [
    "Now we encode the text corpus and store it as PyTorch array. The first 90% of this array will be used for training, while the remaining 10% will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f9a31d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f9a31d1",
    "outputId": "9487d7d2-78ff-4259-e186-4fee08fccaac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens: 858751 Val tokens: 95417\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split: str):\n",
    "    src = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(src) - cfg.block_size - 1, (cfg.batch_size,))\n",
    "    x = torch.stack([src[i:i+cfg.block_size] for i in ix])\n",
    "    y = torch.stack([src[i+1:i+cfg.block_size+1] for i in ix])\n",
    "    return x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "print(\"Train tokens:\", train_data.numel(), \"Val tokens:\", val_data.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b761c",
   "metadata": {
    "id": "ea3b761c"
   },
   "source": [
    "### Model Training Configurations\n",
    "\n",
    "The following configuration classes define the size of the transformer model and how it is trained. These hyperparameters control model capacity, context length, training stability, and computational cost. You can later use this to perform different experiments, e.g. comparing models of different size (capacity) and context length.\n",
    "\n",
    "**Model configuration**\n",
    "\n",
    "- `vocab_size` - number of unique tokens the model can process. In this assignment, tokens are individual characters, so the vocabulary size equals the number of distinct characters in the training corpus.\n",
    "\n",
    "- `block_size` - the context window length, i.e. the maximum number of tokens the model can see at once. During training and generation, the model predicts the next token using only the previous block_size tokens. A larger block size allows the model to capture longer-range dependencies but increases memory and computation requirements.\n",
    "\n",
    "- `n_layer` - number of stacked transformer decoder blocks. More layers increase model depth and expressiveness but also training time and risk of overfitting.\n",
    "\n",
    "- `n_head` - number of attention heads in each self-attention layer. Multiple heads allow the model to attend to different aspects of the context simultaneously (e.g. syntax, formatting, or local patterns).\n",
    "\n",
    "- `n_embd` - dimensionality of token embeddings and hidden representations. Larger embeddings allow richer representations but increase memory usage and compute cost.\n",
    "\n",
    "- `dropout` - dropout probability used during training as a regularization technique. It helps prevent overfitting by randomly deactivating neurons. Dropout is disabled for CPU training to keep behavior deterministic and training stable.\n",
    "\n",
    "**Training configuration**\n",
    "- `batch_size` - Number of training sequences processed in parallel during one optimization step. Larger batches improve gradient stability but require more memory.\n",
    "\n",
    "- `max_iters` - total number of training iterations (parameter update steps).\n",
    "\n",
    "- `eval_interval` - number of training iterations between evaluations on the validation set.\n",
    "\n",
    "- `eval_iters` - number of mini-batches used to estimate training and validation loss during evaluation.\n",
    "\n",
    "- `lr (learning rate)` - step size used by the optimizer when updating model parameters. This is one of the most sensitive hyperparameters for training stability.\n",
    "\n",
    "- `weight_decay` - regularization term that penalizes large weights and helps reduce overfitting.\n",
    "\n",
    "- `device` - specifies whether training runs on CPU or GPU. Smaller model and batch sizes are used automatically when no GPU is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfb2940b",
   "metadata": {
    "id": "cfb2940b"
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "### Model configuration (number of layers, number of heads, embedding dimensions, dropout) ###\n",
    "##############################################################################################\n",
    "\n",
    "# Configuration for GPU\n",
    "@dataclass\n",
    "class ModelConfigGPU:\n",
    "    vocab_size: int\n",
    "    block_size: int\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 256\n",
    "    dropout: float = 0.2\n",
    "\n",
    "# Configuration for CPU\n",
    "@dataclass\n",
    "class ModelConfigCPU:\n",
    "    vocab_size: int\n",
    "    block_size: int\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0\n",
    "\n",
    "########################################################################################################\n",
    "### Model configuration (block size, batch size, evaluation iterations, learning rate, weight decay) ###\n",
    "########################################################################################################\n",
    "\n",
    "# Configuration for GPU\n",
    "@dataclass\n",
    "class TrainConfigGPU:\n",
    "    block_size: int = 256\n",
    "    batch_size: int = 64\n",
    "    max_iters: int = 2000\n",
    "    eval_interval: int = 200\n",
    "    eval_iters: int = 200\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.1\n",
    "    device: str = \"cuda\"\n",
    "\n",
    "# Configuration for CPU\n",
    "@dataclass\n",
    "class TrainConfigCPU:\n",
    "    block_size: int = 64\n",
    "    batch_size: int = 12\n",
    "    max_iters: int = 2000\n",
    "    eval_interval: int = 200\n",
    "    eval_iters: int = 50\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.1\n",
    "    device: str = \"cpu\"\n",
    "    compile: bool = False\n",
    "\n",
    "\n",
    "# Automatically select suitable configuration based on GPU detection\n",
    "if torch.cuda.is_available():\n",
    "  ModelConfig = ModelConfigGPU\n",
    "  TrainConfig = TrainConfigGPU\n",
    "else:\n",
    "  ModelConfig = ModelConfigCPU\n",
    "  TrainConfig = TrainConfigCPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914a8f7",
   "metadata": {
    "id": "6914a8f7"
   },
   "source": [
    "### Building the NanoGPT Model\n",
    "\n",
    "The model begins with token embeddings, which map character indices to dense vectors, and positional embeddings, which encode the order of tokens within a fixed context window (`block_size`). These embeddings are added together and passed through a stack of transformer decoder blocks.\n",
    "\n",
    "Each decoder block consists of two main submodules: causal self-attention and a feed-forward network (MLP). Causal self-attention allows each token to attend only to previous tokens by applying a causal mask, ensuring that the model cannot access future information during training or generation. Multiple attention heads are used so that the model can focus on different aspects of the context in parallel. The MLP applies a non-linear transformation independently at each position, complementing the attention mechanism. Layer normalization and residual connections are used throughout to stabilize training and preserve information flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "GQjzz6O9zVb2",
   "metadata": {
    "id": "GQjzz6O9zVb2"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, c: ModelConfig):\n",
    "        super().__init__()\n",
    "        assert c.n_embd % c.n_head == 0\n",
    "        self.n_head = c.n_head\n",
    "        self.head_dim = c.n_embd // c.n_head\n",
    "\n",
    "        self.qkv = nn.Linear(c.n_embd, 3 * c.n_embd, bias=False)\n",
    "        self.proj = nn.Linear(c.n_embd, c.n_embd, bias=False)\n",
    "        self.dropout = nn.Dropout(c.dropout)\n",
    "\n",
    "        mask = torch.tril(torch.ones(c.block_size, c.block_size)).view(1, 1, c.block_size, c.block_size)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.proj(y)\n",
    "        y = self.dropout(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, c: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(c.n_embd, 4 * c.n_embd)\n",
    "        self.proj = nn.Linear(4 * c.n_embd, c.n_embd)\n",
    "        self.dropout = nn.Dropout(c.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, c: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(c.n_embd)\n",
    "        self.attn = CausalSelfAttention(c)\n",
    "        self.ln2 = nn.LayerNorm(c.n_embd)\n",
    "        self.mlp = MLP(c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cgvSRhqr1Ltu",
   "metadata": {
    "id": "cgvSRhqr1Ltu"
   },
   "source": [
    "After passing through all decoder blocks, the model applies a final layer normalization and a linear output head that produces a probability distribution over the vocabulary for the next-token prediction. The training objective is standard cross-entropy loss between the predicted next token and the true next token.\n",
    "\n",
    "This NanoGPT model contains approximately between 0.8 and 3 million parameters, depending on the configuration. By comparison, GPT-2 contains hundreds of millions of parameters and modern commercial language models contain many billion parameters. Despite this difference in scale, the same architectural principles apply. The total number of parameters is mainly determined by the embedding dimension (`n_embd`), the number of layers (`n_layer`), and the vocabulary size, while the number of attention heads controls how the embedding space is partitioned rather than its overall size.\n",
    "\n",
    "The model is intentionally kept small so that it can be trained quickly on limited hardware and easily inspected. The goal of this assignment is to understand how transformer components interact, not to achieve state-of-the-art language modeling performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6762045",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6762045",
    "outputId": "61207d85-c6df-4254-c541-40a1fbbf12f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 0.827136 M\n"
     ]
    }
   ],
   "source": [
    "class NanoGPT(nn.Module):\n",
    "    def __init__(self, c: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.c = c\n",
    "        self.tok_emb = nn.Embedding(c.vocab_size, c.n_embd)\n",
    "        self.pos_emb = nn.Embedding(c.block_size, c.n_embd)\n",
    "        self.drop = nn.Dropout(c.dropout)\n",
    "        self.blocks = nn.ModuleList([Block(c) for _ in range(c.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(c.n_embd)\n",
    "        self.head = nn.Linear(c.n_embd, c.vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.c.block_size\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
    "        x = self.drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "cfg = TrainConfig()\n",
    "mcfg = ModelConfig(vocab_size=vocab_size, block_size=cfg.block_size)\n",
    "\n",
    "model = NanoGPT(mcfg).to(cfg.device)\n",
    "print(\"Parameters:\", sum(p.numel() for p in model.parameters())/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ec49d",
   "metadata": {
    "id": "3a8ec49d"
   },
   "source": [
    "### The Training Loop\n",
    "\n",
    "We use the AdamW optimizer and periodically evaluate on the validation set. The training in Google Colab should take for both the CPU and GPU configurations approximately 4-5 minutes using the baseline configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dd3b1d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ab3cc7944baa486a913f74946ea7ca3e",
      "4a9c236c35e340e0bdb9357a75f2eb26",
      "bcc336460a8e422297810c36ef64a9b3",
      "b9d6948fcb5d487aba3e4b91ed9473d1",
      "5d378a9cb39146b0a9e9819ef93e93c4",
      "306308fcc9af4342abce2d7e262049f4",
      "81c9e9571aa542b592779f3ab52e12ac",
      "bffcb9d6506a47659a28563510fdf986",
      "79b80301f3064ef0ad855eeebf2c9712",
      "405899e53d7b48909caddb41124b23f5",
      "9d049885cae340629e18319e5bd4cf84"
     ]
    },
    "id": "4dd3b1d8",
    "outputId": "7d40f7fc-b83e-4342-de6f-c843f6170ea9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a3fcc019ff49dab7df2b6b13de0de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(cfg.eval_iters)\n",
    "        for k in range(cfg.eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "model.train()\n",
    "pbar = tqdm(range(cfg.max_iters), desc=\"training\")\n",
    "for it in pbar:\n",
    "    if it % cfg.eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        pbar.set_postfix(train=losses[\"train\"], val=losses[\"val\"])\n",
    "\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    _, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6af6d0",
   "metadata": {
    "id": "3b6af6d0"
   },
   "source": [
    "### Text Generation (Sampling)\n",
    "\n",
    "Now that the model is trained we can put it to use. We generate the synthetic captions starting from `\"H&E stained section showing\"` string by autoregressively sampling next characters\n",
    "\n",
    "Hyperparameters:\n",
    "- `temperature`: higher = more random, lower = less variety\n",
    "- `top_k`: restrict sampling to top-k most likely chars (lower number eliminates less likely candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7856711",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7856711",
    "outputId": "b4cded5d-a5b8-4a27-c6de-ec30361b4026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H&E stained section showing epidermota in the basitation in the infiltraton in a atypical melanocytic nevus, and higher magnification of 200X magnification of ×1, and a papillary in the epidermis of a 25 μm. Hematoxylin and Eosin (H&E) and at ×40 magnification. Histopathological findings a predominanthomatosis (the extensive and to the pigmented pathological atypical cells are and seevenitifing. The image is stained with Haematoxylin and Eosin (H&E) at ×400. Histopathological finding the pidermatocytoplasm. The image is i\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(start: str, max_new_tokens=400, temperature=1.0, top_k=60):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([encode(start)], dtype=torch.long, device=cfg.device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -cfg.block_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, k=min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "prompt = \"H&E stained section showing\"\n",
    "\n",
    "print(generate(prompt, max_new_tokens=500, temperature=0.7, top_k=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f88fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for exercise 1 (not required but we wanted to plot the losses just to make sure)\n",
    "EX1_LOG_FILE = \"ex1_experiments.json\"\n",
    "\n",
    "def run_ex1_experiment(sep=\"\\n<ENDC>\\n\", label=\"baseline\"):\n",
    "    ex1_text = sep.join(captions)\n",
    "    ex1_chars = sorted(list(set(ex1_text)))\n",
    "    ex1_vs = len(ex1_chars)\n",
    "    ex1_stoi = {ch: i for i, ch in enumerate(ex1_chars)}\n",
    "    ex1_itos = {i: ch for i, ch in enumerate(ex1_chars)}\n",
    "    ex1_encode = lambda s: [ex1_stoi[c] for c in s]\n",
    "    ex1_decode = lambda ids: \"\".join(ex1_itos[i] for i in ids)\n",
    "\n",
    "    ex1_data = torch.tensor(ex1_encode(ex1_text), dtype=torch.long)\n",
    "    n = int(0.9 * len(ex1_data))\n",
    "    ex1_train, ex1_val = ex1_data[:n], ex1_data[n:]\n",
    "\n",
    "    ex1_mcfg = ModelConfig(vocab_size=ex1_vs, block_size=cfg.block_size)\n",
    "    m = NanoGPT(ex1_mcfg).to(cfg.device)\n",
    "    opt = torch.optim.AdamW(m.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    def _get_batch(split):\n",
    "        src = ex1_train if split == \"train\" else ex1_val\n",
    "        ix = torch.randint(len(src) - cfg.block_size - 1, (cfg.batch_size,))\n",
    "        x = torch.stack([src[i:i+cfg.block_size] for i in ix])\n",
    "        y = torch.stack([src[i+1:i+cfg.block_size+1] for i in ix])\n",
    "        return x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "    eval_steps, train_losses, val_losses = [], [], []\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _est():\n",
    "        m.eval()\n",
    "        out = {}\n",
    "        for sp in [\"train\", \"val\"]:\n",
    "            ls = torch.zeros(cfg.eval_iters)\n",
    "            for k in range(cfg.eval_iters):\n",
    "                X, Y = _get_batch(sp)\n",
    "                _, l = m(X, Y)\n",
    "                ls[k] = l.item()\n",
    "            out[sp] = ls.mean().item()\n",
    "        m.train()\n",
    "        return out\n",
    "\n",
    "    m.train()\n",
    "    pbar = tqdm(range(cfg.max_iters), desc=f\"Ex1: {label}\")\n",
    "    for it in pbar:\n",
    "        if it % cfg.eval_interval == 0:\n",
    "            losses = _est()\n",
    "            eval_steps.append(it)\n",
    "            train_losses.append(losses[\"train\"])\n",
    "            val_losses.append(losses[\"val\"])\n",
    "            pbar.set_postfix(train=f\"{losses['train']:.4f}\", val=f\"{losses['val']:.4f}\")\n",
    "        xb, yb = _get_batch(\"train\")\n",
    "        _, loss = m(xb, yb)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    final = _est()\n",
    "    eval_steps.append(cfg.max_iters)\n",
    "    train_losses.append(final[\"train\"])\n",
    "    val_losses.append(final[\"val\"])\n",
    "\n",
    "    m.eval()\n",
    "    idx = torch.tensor([ex1_encode(\"H&E stained section showing\")], dtype=torch.long, device=cfg.device)\n",
    "    for _ in range(500):\n",
    "        idx_cond = idx[:, -cfg.block_size:]\n",
    "        logits, _ = m(idx_cond)\n",
    "        logits = logits[:, -1, :] / 0.7\n",
    "        v, _ = torch.topk(logits, k=min(60, logits.size(-1)))\n",
    "        logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    generated = ex1_decode(idx[0].tolist())\n",
    "\n",
    "    result = {\n",
    "        \"label\": label, \"sep\": sep, \"vocab_size\": ex1_vs,\n",
    "        \"eval_steps\": eval_steps, \"train_losses\": train_losses, \"val_losses\": val_losses,\n",
    "        \"final_train_loss\": final[\"train\"], \"final_val_loss\": final[\"val\"],\n",
    "        \"generated_sample\": generated,\n",
    "    }\n",
    "\n",
    "    with open(EX1_LOG_FILE, \"r\") as f: log = json.load(f)\n",
    "    log.append(result)\n",
    "    with open(EX1_LOG_FILE, \"w\") as f: json.dump(log, f, indent=2)\n",
    "    print(f\"[{label}] vocab={ex1_vs} train={final['train']:.4f} val={final['val']:.4f}\")\n",
    "    print(f\"Sample: {generated[:200]}\")\n",
    "    return result\n",
    "\n",
    "def draw_ex1_plots():\n",
    "    with open(EX1_LOG_FILE, \"r\") as f: log = json.load(f)\n",
    "    print(f\"{'Label':<25} {'Vocab':<8} {'Train Loss':<12} {'Val Loss':<12}\")\n",
    "    for exp in log:\n",
    "        print(f\"{exp['label']:<25} {exp['vocab_size']:<8} {exp['final_train_loss']:<12.4f} {exp['final_val_loss']:<12.4f}\")\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "    colors = plt.cm.tab10.colors\n",
    "    for i, exp in enumerate(log):\n",
    "        c = colors[i % len(colors)]\n",
    "        ax.plot(exp[\"eval_steps\"], exp[\"train_losses\"], color=c, linestyle=\"-\", label=f'{exp[\"label\"]} (train)')\n",
    "        ax.plot(exp[\"eval_steps\"], exp[\"val_losses\"], color=c, linestyle=\"--\", label=f'{exp[\"label\"]} (val)')\n",
    "    ax.set_xlabel(\"Training Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Exercise 1: Effect of Separator on Training\")\n",
    "    ax.legend(fontsize=\"small\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"ex1_loss_plots.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"\\nGenerated Samples:\")\n",
    "    for exp in log:\n",
    "        print(f\"\\n[{exp['label']}]:\")\n",
    "        print(exp[\"generated_sample\"][:300])\n",
    "\n",
    "def reproduce_ex1():\n",
    "    with open(EX1_LOG_FILE, \"w\") as f: json.dump([], f)\n",
    "    run_ex1_experiment(sep=\"\\n<ENDC>\\n\", label=\"sep=<ENDC>\")\n",
    "    run_ex1_experiment(sep=\" \", label='sep=\" \"')\n",
    "    run_ex1_experiment(sep=\"\", label='sep=\"\"')\n",
    "    draw_ex1_plots()\n",
    "\n",
    "reproduce_ex1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3Gw-PZ8Pgl1a",
   "metadata": {
    "id": "3Gw-PZ8Pgl1a"
   },
   "outputs": [],
   "source": [
    "#exercise 2 helpers, train model and log results, and plot them\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import replace as _dc_replace\n",
    "\n",
    "EX2_LOG_FILE = \"ex2_experiments.json\"\n",
    "\n",
    "def load_ex2_log():\n",
    "    with open(EX2_LOG_FILE, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_ex2_log(log):\n",
    "    with open(EX2_LOG_FILE, \"w\") as f:\n",
    "        json.dump(log, f, indent=2)\n",
    "\n",
    "#utilities for our own use, not for grading\n",
    "def list_experiments():\n",
    "    log = load_ex2_log()\n",
    "    for i, exp in enumerate(log):\n",
    "        hp = exp[\"hyperparameters\"]\n",
    "        print(f\"  [{i}] {exp['label']:<25} train={exp['final_train_loss']:.4f}  val={exp['final_val_loss']:.4f}\")\n",
    "\n",
    "def delete_experiment(index):\n",
    "    log = load_ex2_log()\n",
    "    removed = log.pop(index)\n",
    "    save_ex2_log(log)\n",
    "    print(f\"Deleted: '{removed['label']}'. Remaining: {len(log)} experiments.\")\n",
    "\n",
    "def clear_all_experiments():\n",
    "    save_ex2_log([])\n",
    "    print(\"All experiments cleared.\")\n",
    "\n",
    "\n",
    "def run_ex2_experiment(lr=1e-3, batch_size=12, eval_interval=200,\n",
    "                       max_iters=2000, eval_iters=50, label=None):\n",
    "    if label is None:\n",
    "        parts = []\n",
    "        if lr != 1e-3: parts.append(f\"lr={lr}\")\n",
    "        if batch_size != 12: parts.append(f\"bs={batch_size}\")\n",
    "        if eval_interval != 200: parts.append(f\"eval_int={eval_interval}\")\n",
    "        label = \", \".join(parts) if parts else \"baseline\"\n",
    "\n",
    "    # Local config copy , we dont change global cfg deliberately\n",
    "    ecfg = _dc_replace(cfg, lr=lr, batch_size=batch_size,\n",
    "                        eval_interval=eval_interval, max_iters=max_iters,\n",
    "                        eval_iters=eval_iters)\n",
    "\n",
    "    def _get_batch(split):\n",
    "        src = train_data if split == \"train\" else val_data\n",
    "        ix = torch.randint(len(src) - ecfg.block_size - 1, (ecfg.batch_size,))\n",
    "        x = torch.stack([src[i:i+ecfg.block_size] for i in ix])\n",
    "        y = torch.stack([src[i+1:i+ecfg.block_size+1] for i in ix])\n",
    "        return x.to(ecfg.device), y.to(ecfg.device)\n",
    "\n",
    "    fresh_model = NanoGPT(mcfg).to(ecfg.device)\n",
    "    opt = torch.optim.AdamW(fresh_model.parameters(), lr=ecfg.lr, weight_decay=ecfg.weight_decay)\n",
    "\n",
    "    eval_steps = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _estimate_loss():\n",
    "        fresh_model.eval()\n",
    "        out = {}\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            losses_t = torch.zeros(ecfg.eval_iters)\n",
    "            for k in range(ecfg.eval_iters):\n",
    "                X, Y = _get_batch(split)\n",
    "                _, l = fresh_model(X, Y)\n",
    "                losses_t[k] = l.item()\n",
    "            out[split] = losses_t.mean().item()\n",
    "        fresh_model.train()\n",
    "        return out\n",
    "\n",
    "    fresh_model.train()\n",
    "    pbar = tqdm(range(ecfg.max_iters), desc=f\"Ex2: {label}\")\n",
    "    for it in pbar:\n",
    "        if it % ecfg.eval_interval == 0:\n",
    "            losses = _estimate_loss()\n",
    "            eval_steps.append(it)\n",
    "            train_losses.append(losses[\"train\"])\n",
    "            val_losses.append(losses[\"val\"])\n",
    "            pbar.set_postfix(train=f\"{losses['train']:.4f}\", val=f\"{losses['val']:.4f}\")\n",
    "\n",
    "        xb, yb = _get_batch(\"train\")\n",
    "        _, loss = fresh_model(xb, yb)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    final = _estimate_loss()\n",
    "    eval_steps.append(ecfg.max_iters)\n",
    "    train_losses.append(final[\"train\"])\n",
    "    val_losses.append(final[\"val\"])\n",
    "\n",
    "    result = {\n",
    "        \"label\": label,\n",
    "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "        \"hyperparameters\": {\n",
    "            \"lr\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"eval_interval\": eval_interval,\n",
    "            \"max_iters\": max_iters,\n",
    "            \"eval_iters\": eval_iters,\n",
    "        },\n",
    "        \"eval_steps\": eval_steps,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"final_train_loss\": final[\"train\"],\n",
    "        \"final_val_loss\": final[\"val\"],\n",
    "    }\n",
    "\n",
    "    log = load_ex2_log()\n",
    "    log.append(result)\n",
    "    save_ex2_log(log)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def draw_plots(log_file=\"\"):\n",
    "    if log_file:\n",
    "        with open(log_file, \"r\") as f:\n",
    "            log = json.load(f)\n",
    "    else:\n",
    "        log = load_ex2_log()\n",
    "    if not log:\n",
    "        print(\"No experiments logged yet\")\n",
    "    else:\n",
    "        print(f\"{'Label':<25} {'LR':<10} {'BS':<6} {'Eval Int':<10} {'Train Loss':<12} {'Val Loss':<12}\")\n",
    "        for exp in log:\n",
    "            hp = exp[\"hyperparameters\"]\n",
    "            print(f\"{exp['label']:<25} {hp['lr']:<10} {hp['batch_size']:<6} \"\n",
    "                f\"{hp['eval_interval']:<10} {exp['final_train_loss']:<12.4f} {exp['final_val_loss']:<12.4f}\")\n",
    "\n",
    "        def categorize(exp):\n",
    "            hp = exp[\"hyperparameters\"]\n",
    "            cats = []\n",
    "            if hp[\"lr\"] != 1e-3 or exp[\"label\"] == \"baseline\": cats.append(\"learning_rate\")\n",
    "            if hp[\"batch_size\"] != 12 or exp[\"label\"] == \"baseline\": cats.append(\"batch_size\")\n",
    "            if hp[\"eval_interval\"] != 200 or exp[\"label\"] == \"baseline\": cats.append(\"eval_interval\")\n",
    "            if not cats: cats = [\"other\"]\n",
    "            return cats\n",
    "\n",
    "        groups = {\"learning_rate\": [], \"batch_size\": [], \"eval_interval\": [], \"other\": []}\n",
    "        for exp in log:\n",
    "            for cat in categorize(exp):\n",
    "                if cat in groups:\n",
    "                    groups[cat].append(exp)\n",
    "\n",
    "        groups = {k: v for k, v in groups.items() if v}\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(groups), figsize=(7 * len(groups), 5), squeeze=False)\n",
    "\n",
    "        colors = plt.cm.tab10.colors\n",
    "        group_titles = {\n",
    "            \"learning_rate\": \"Effect of Learning Rate\",\n",
    "            \"batch_size\": \"Effect of Batch Size\",\n",
    "            \"eval_interval\": \"Effect of Evaluation Interval\",\n",
    "        }\n",
    "\n",
    "        for ax_idx, (group_name, exps) in enumerate(groups.items()):\n",
    "            ax = axes[0][ax_idx]\n",
    "            for i, exp in enumerate(exps):\n",
    "                c = colors[i % len(colors)]\n",
    "                ax.plot(exp[\"eval_steps\"], exp[\"train_losses\"],\n",
    "                        color=c, linestyle=\"-\", label=f'{exp[\"label\"]} (train)')\n",
    "                ax.plot(exp[\"eval_steps\"], exp[\"val_losses\"],\n",
    "                        color=c, linestyle=\"--\", label=f'{exp[\"label\"]} (val)')\n",
    "            ax.set_xlabel(\"Training Iteration\")\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "            ax.set_title(group_titles.get(group_name, group_name))\n",
    "            ax.legend(fontsize=\"small\")\n",
    "        plt.tight_layout()\n",
    "        save_name = os.path.splitext(log_file)[0] + \".png\" if log_file else \"ex2_loss_plots.png\"\n",
    "        plt.savefig(save_name, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        print(f\"Plot saved to {save_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UBeyOGG-Uoz1",
   "metadata": {
    "id": "UBeyOGG-Uoz1"
   },
   "source": [
    "Exercise 2 experiment runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C3povF-YhJL4",
   "metadata": {
    "id": "C3povF-YhJL4"
   },
   "outputs": [],
   "source": [
    "\n",
    "#two blocks of experiment calls for demonstrating average changes and extreme changes for exercise 2\n",
    "\n",
    "def reproduce_average_changes():\n",
    "    clear_all_experiments()\n",
    "    run_ex2_experiment() #1. baseline\n",
    "    #average changes:\n",
    "    run_ex2_experiment(lr=1e-4, label=\"lr=1e-4 (low)\") # 2. Low learning rate\n",
    "    run_ex2_experiment(lr=1e-2, label=\"lr=1e-2 (high)\") # 3. High learning rate\n",
    "    run_ex2_experiment(batch_size=4, label=\"bs=4 (small)\") # 4. Small batch size\n",
    "    run_ex2_experiment(batch_size=32, label=\"bs=32 (large)\") # 5. Large batch size\n",
    "    run_ex2_experiment(eval_interval=50, label=\"eval_int=50 (freq)\") # 6. Frequent eval\n",
    "    run_ex2_experiment(eval_interval=500, label=\"eval_int=500 (rare)\") # 7. Rare eval\n",
    "    draw_plots()\n",
    "\n",
    "def reproduce_extreme_changes():\n",
    "    clear_all_experiments()\n",
    "    run_ex2_experiment()\n",
    "    #extreme changes:\n",
    "    run_ex2_experiment(lr=1e-5, label=\"lr=1e-5 (very low)\") # 2. Very low learning rate\n",
    "    run_ex2_experiment(lr=5e-2, label=\"lr=5e-2 (very high)\") # 3. Very high learning rate\n",
    "    run_ex2_experiment(batch_size=2, label=\"bs=2 (tiny)\") # 4. Tiny batch size\n",
    "    run_ex2_experiment(batch_size=64, label=\"bs=64 (very large)\") # 5. Very large batch size\n",
    "    run_ex2_experiment(eval_interval=25, label=\"eval_int=25 (freq)\") # 6. Very frequent eval\n",
    "    run_ex2_experiment(eval_interval=1000, label=\"eval_int=1000 (rare)\") # 7. Very rare eval\n",
    "    draw_plots()\n",
    "\n",
    "\n",
    "reproduce_average_changes() # call this if you want to reproduce our experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "E0RANaaLu6Tm",
   "metadata": {
    "id": "E0RANaaLu6Tm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ex3 experiments cleared.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129dc1007e3b4345a5fc1c15efd3bf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ex3: both (baseline):   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ex3 experiments cleared.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129dc1007e3b4345a5fc1c15efd3bf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ex3: both (baseline):   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[both (baseline)] vocab=108 train=1.0547 val=1.1631\n",
      "Sample: H&E stained section showing moderate. The eassed of tucout cells with bructatter butek. Character dermis. Haus distintis as as eosinophilic ch tumor irrhagin the lymphocytic inflammatory strea. Areasi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1153de23f2e24876ad206fc3269757a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ex3: lowercase:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ex3 experiments cleared.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129dc1007e3b4345a5fc1c15efd3bf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ex3: both (baseline):   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[both (baseline)] vocab=108 train=1.0547 val=1.1631\n",
      "Sample: H&E stained section showing moderate. The eassed of tucout cells with bructatter butek. Character dermis. Haus distintis as as eosinophilic ch tumor irrhagin the lymphocytic inflammatory strea. Areasi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1153de23f2e24876ad206fc3269757a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ex3: lowercase:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lowercase] vocab=82 train=1.0856 val=1.1430\n",
      "Sample: h&e stained section showing was a distibuterfitinclumelamous termbedderetined it the ra 6-ya or of by thistopathological melanoma, markound to fost suggineds. the image is characted formatotic ircuded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1c7e43fe2940ed95f368b4a8ba065a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ex3: uppercase:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ex3 experiments cleared.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129dc1007e3b4345a5fc1c15efd3bf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ex3: both (baseline):   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[both (baseline)] vocab=108 train=1.0547 val=1.1631\n",
      "Sample: H&E stained section showing moderate. The eassed of tucout cells with bructatter butek. Character dermis. Haus distintis as as eosinophilic ch tumor irrhagin the lymphocytic inflammatory strea. Areasi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1153de23f2e24876ad206fc3269757a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ex3: lowercase:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lowercase] vocab=82 train=1.0856 val=1.1430\n",
      "Sample: h&e stained section showing was a distibuterfitinclumelamous termbedderetined it the ra 6-ya or of by thistopathological melanoma, markound to fost suggineds. the image is characted formatotic ircuded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1c7e43fe2940ed95f368b4a8ba065a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ex3: uppercase:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[uppercase] vocab=81 train=1.0841 val=1.1874\n",
      "Sample: H&E STAINED SECTION SHOWING A PROGID A DENOCRINE OF THE IN A PEVERAL AYRE ARRISTIC OF THE NEVUS MALE PESITCULAR SEMBENTATION A 4103.M IS POWTOMORPHISM. HISTOPATHOLOGY, SHOWING A TROUND NODULE MULTINE \n",
      "Label                     Case     Vocab    Train Loss   Val Loss    \n",
      "both (baseline)           both     108      1.0547       1.1631      \n",
      "lowercase                 lower    82       1.0856       1.1430      \n",
      "uppercase                 upper    81       1.0841       1.1874      \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ex3_loss_plots.png\n",
      "\n",
      " Generated Samples\n",
      "\n",
      "[both (baseline)]:\n",
      "H&E stained section showing moderate. The eassed of tucout cells with bructatter butek. Character dermis. Haus distintis as as eosinophilic ch tumor irrhagin the lymphocytic inflammatory strea. Areasiving the histopathology ilike with atypical cells, sust a gircoscncoment is an with feature oreticin\n",
      "\n",
      "[lowercase]:\n",
      "h&e stained section showing was a distibuterfitinclumelamous termbedderetined it the ra 6-ya or of by thistopathological melanoma, markound to fost suggineds. the image is characted formatotic ircuded spr‘tin (hc) of collagensative large prineural-de), and mordule invased at imorma-blame, nevus, a b\n",
      "\n",
      "[uppercase]:\n",
      "H&E STAINED SECTION SHOWING A PROGID A DENOCRINE OF THE IN A PEVERAL AYRE ARRISTIC OF THE NEVUS MALE PESITCULAR SEMBENTATION A 4103.M IS POWTOMORPHISM. HISTOPATHOLOGY, SHOWING A TROUND NODULE MULTINE WITH A COLLAGENOUS CHARACTLY TKERATIC ASYTIC SYRINGTUICALLY MELANOMA, SHOWING FIBRINGS OF THE NESTS \n"
     ]
    }
   ],
   "source": [
    "#exercise 3\n",
    "EX3_LOG_FILE = \"ex3_experiments.json\"\n",
    "\n",
    "def preprocessed_text(text, case_mode=\"both\"):\n",
    "    if case_mode == \"lower\":\n",
    "        return text.lower()\n",
    "    elif case_mode == \"upper\":\n",
    "        return text.upper()\n",
    "    return text\n",
    "\n",
    "def load_ex3_log():\n",
    "    with open(EX3_LOG_FILE, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def save_ex3_log(log):\n",
    "    with open(EX3_LOG_FILE, \"w\") as f:\n",
    "        json.dump(log, f, indent=2)\n",
    "\n",
    "def list_ex3_experiments():\n",
    "    log = load_ex3_log()\n",
    "    for i, exp in enumerate(log):\n",
    "        print(f\"  [{i}] {exp['label']:<25} train={exp['final_train_loss']:.4f}  val={exp['final_val_loss']:.4f}\")\n",
    "\n",
    "def clear_all_ex3_experiments():\n",
    "    save_ex3_log([])\n",
    "    print(\"All ex3 experiments cleared.\")\n",
    "\n",
    "\n",
    "def run_ex3_experiment(case_mode=\"both\", label=None):\n",
    "    if label is None:\n",
    "        label = f\"case={case_mode}\"\n",
    "\n",
    "    ex3_text = preprocessed_text(SEP.join(captions), case_mode)\n",
    "    ex3_chars = sorted(list(set(ex3_text)))\n",
    "    ex3_vocab_size = len(ex3_chars)\n",
    "    ex3_stoi = {ch: i for i, ch in enumerate(ex3_chars)}\n",
    "    ex3_itos = {i: ch for i, ch in enumerate(ex3_chars)}\n",
    "    ex3_encode = lambda s: [ex3_stoi[c] for c in s]\n",
    "    ex3_decode = lambda ids: \"\".join(ex3_itos[i] for i in ids)\n",
    "\n",
    "    ex3_data = torch.tensor(ex3_encode(ex3_text), dtype=torch.long)\n",
    "    n = int(0.9 * len(ex3_data))\n",
    "    ex3_train = ex3_data[:n]\n",
    "    ex3_val = ex3_data[n:]\n",
    "\n",
    "    ex3_mcfg = ModelConfig(vocab_size=ex3_vocab_size, block_size=cfg.block_size)\n",
    "    fresh_model = NanoGPT(ex3_mcfg).to(cfg.device)\n",
    "    opt = torch.optim.AdamW(fresh_model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    def _get_batch(split):\n",
    "        src = ex3_train if split == \"train\" else ex3_val\n",
    "        ix = torch.randint(len(src) - cfg.block_size - 1, (cfg.batch_size,))\n",
    "        x = torch.stack([src[i:i+cfg.block_size] for i in ix])\n",
    "        y = torch.stack([src[i+1:i+cfg.block_size+1] for i in ix])\n",
    "        return x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "    eval_steps = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _estimate_loss():\n",
    "        fresh_model.eval()\n",
    "        out = {}\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            losses_t = torch.zeros(cfg.eval_iters)\n",
    "            for k in range(cfg.eval_iters):\n",
    "                X, Y = _get_batch(split)\n",
    "                _, l = fresh_model(X, Y)\n",
    "                losses_t[k] = l.item()\n",
    "            out[split] = losses_t.mean().item()\n",
    "        fresh_model.train()\n",
    "        return out\n",
    "\n",
    "    fresh_model.train()\n",
    "    pbar = tqdm(range(cfg.max_iters), desc=f\"Ex3: {label}\")\n",
    "    for it in pbar:\n",
    "        if it % cfg.eval_interval == 0:\n",
    "            losses = _estimate_loss()\n",
    "            eval_steps.append(it)\n",
    "            train_losses.append(losses[\"train\"])\n",
    "            val_losses.append(losses[\"val\"])\n",
    "            pbar.set_postfix(train=f\"{losses['train']:.4f}\", val=f\"{losses['val']:.4f}\")\n",
    "\n",
    "        xb, yb = _get_batch(\"train\")\n",
    "        _, loss = fresh_model(xb, yb)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    final = _estimate_loss()\n",
    "    eval_steps.append(cfg.max_iters)\n",
    "    train_losses.append(final[\"train\"])\n",
    "    val_losses.append(final[\"val\"])\n",
    "\n",
    "    prompt_raw = \"H&E stained section showing\"\n",
    "    prompt = preprocessed_text(prompt_raw, case_mode)\n",
    "\n",
    "    fresh_model.eval()\n",
    "    idx = torch.tensor([ex3_encode(prompt)], dtype=torch.long, device=cfg.device)\n",
    "    temperature = 1.0\n",
    "    top_k = 60\n",
    "    for _ in range(500):\n",
    "        idx_cond = idx[:, -cfg.block_size:]\n",
    "        logits, _ = fresh_model(idx_cond)\n",
    "        logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, k=min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    generated = ex3_decode(idx[0].tolist())\n",
    "\n",
    "    result = {\n",
    "        \"label\": label,\n",
    "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "        \"case_mode\": case_mode,\n",
    "        \"vocab_size\": ex3_vocab_size,\n",
    "        \"eval_steps\": eval_steps,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"final_train_loss\": final[\"train\"],\n",
    "        \"final_val_loss\": final[\"val\"],\n",
    "        \"generated_sample\": generated,\n",
    "    }\n",
    "\n",
    "    log = load_ex3_log()\n",
    "    log.append(result)\n",
    "    save_ex3_log(log)\n",
    "    print(f\"[{label}] vocab={ex3_vocab_size} train={final['train']:.4f} val={final['val']:.4f}\")\n",
    "    print(f\"Sample: {generated[:200]}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def draw_ex3_plots(log_file=\"\"):\n",
    "    if log_file:\n",
    "        with open(log_file, \"r\") as f:\n",
    "            log = json.load(f)\n",
    "    else:\n",
    "        log = load_ex3_log()\n",
    "    if not log:\n",
    "        print(\"No ex3 experiments logged yet\")\n",
    "        return\n",
    "\n",
    "    print(f\"{'Label':<25} {'Case':<8} {'Vocab':<8} {'Train Loss':<12} {'Val Loss':<12}\")\n",
    "    for exp in log:\n",
    "        print(f\"{exp['label']:<25} {exp['case_mode']:<8} {exp['vocab_size']:<8} \"\n",
    "              f\"{exp['final_train_loss']:<12.4f} {exp['final_val_loss']:<12.4f}\")\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "    colors = plt.cm.tab10.colors\n",
    "    for i, exp in enumerate(log):\n",
    "        c = colors[i % len(colors)]\n",
    "        ax.plot(exp[\"eval_steps\"], exp[\"train_losses\"],\n",
    "                color=c, linestyle=\"-\", label=f'{exp[\"label\"]} (train)')\n",
    "        ax.plot(exp[\"eval_steps\"], exp[\"val_losses\"],\n",
    "                color=c, linestyle=\"--\", label=f'{exp[\"label\"]} (val)')\n",
    "    ax.set_xlabel(\"Training Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Exercise 3: Effect of Case Mode on Training\")\n",
    "    ax.legend(fontsize=\"small\")\n",
    "    plt.tight_layout()\n",
    "    save_name = os.path.splitext(log_file)[0] + \".png\" if log_file else \"ex3_loss_plots.png\"\n",
    "    plt.savefig(save_name, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Plot saved to {save_name}\")\n",
    "\n",
    "    print(\"\\n Generated Samples\")\n",
    "    for exp in log:\n",
    "        print(f\"\\n[{exp['label']}]:\")\n",
    "        print(exp[\"generated_sample\"][:300])\n",
    "\n",
    "\n",
    "def reproduce_ex3():\n",
    "    clear_all_ex3_experiments()\n",
    "    run_ex3_experiment(case_mode=\"both\", label=\"both (baseline)\")\n",
    "    run_ex3_experiment(case_mode=\"lower\", label=\"lowercase\")\n",
    "    run_ex3_experiment(case_mode=\"upper\", label=\"uppercase\")\n",
    "    draw_ex3_plots()\n",
    "\n",
    "reproduce_ex3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HGlVHOq4RNzJ",
   "metadata": {
    "id": "HGlVHOq4RNzJ"
   },
   "source": [
    "Exercise 5 code can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "lxKgTPLHRNUW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxKgTPLHRNUW",
    "outputId": "27427ed2-b4a8-4b34-fd40-97fdab8c2414"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here stats a new generated prompt with temperature = 0.4 and top_k = 60\n",
      "H&E stained section showing a component of the responding the tumor melanoma (SL) of the paredominantly and eye of the patient and a dermis. The image is shows a predominantly pigmented nests and subcution of the surrroscomastic tumounted tumor a of a 2000x magnification. Histopathology of a image of stained with Hematoxylin and Eosin (H&E) at × 2000x magnification. Histopathology of a tumor cell cells with a predominantly continal patient with a subcutaneous and patient in the semetastating a dermis and a predomination t\n",
      "\n",
      "here stats a new generated prompt with temperature = 0.5 and top_k = 60\n",
      "H&E stained section showing a betwen as epithelioid a minated spindle cell cells and the subcuted in the nevus predominantly the dermis. This a a predominantly a dermal distinct multinucleation infiltration a porifature and in the epithelium and capt large at the dermis, are tumor stained with Hematoxylin and Eosin (H&E). Histopathology of a a betwo pale image showing with predominantly displaying the presentation of the dermal pattern of a deep tese of the dermis a subcuting a betwen of the base of tumor the dermatous (S\n",
      "\n",
      "here stats a new generated prompt with temperature = 0.75 and top_k = 60\n",
      "H&E stained section showing malign pigmentation of the submitting a melanoma (H&E, original magnification 100×). Histopathological findings suresparating a pedimations. Histopathology showing a histopathology of the sebaceous, a gepidermatotic and componention at and lymphocytic follicular dermal patients and dermatofinged melanoma (DFM) the reverly lesion of the study in a structures of the undiffication infiltrating the landermis. There lesion and pigmented minitial postation of a 2000%). The image is shows an infiltrat\n",
      "\n",
      "here starts a newly generated prompt with temperature = 0.4 and top_k = 60\n",
      "H&E stained section showing a betwen area of a papillomatory of the presentation of the dermal predominantly the cell cells are and as a presenting in the subition of a 2000x magnification. Histopathology of a a betwen and signification of the a presented in the periletion infiltration a presenting the sebecuous corring a showing a melanoma (PCM) an a 2000x magnification. Histopathology of a dematoxylin and eosin (H&E) showing a a some tumour such a in the submittis. The image is stained with Hematoxylin and Eosin (H&E) a\n",
      "\n",
      "here starts a newly generated prompt with temperature = 0.4 and top_k = 30\n",
      "H&E stained section showing a cells area briskin a superficial dermal postive culting a melanocytes and in the presenting the submitting a tumor a strix cell cystic cells are in the dermatopathological compond spindle cell cell nuclei and eosinophilic cystic cell cells are presented in the surrounding the dermatopathology of a mitotic activity a infiltration a and superficial spindle-sitotic active and in the submitting a be a presented in the dermal dermal pattern of the dermal pigmented in the atypical cells are atypica\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_ex5(prompt, list_of_temperature = [1,0,10], list_of_top_k = [60,1]):\n",
    "  for i in range(len(list_of_temperature)):\n",
    "    print(f\"here stats a new generated prompt with temperature = {list_of_temperature[i]} and top_k = {list_of_top_k[0]}\")\n",
    "    print(generate(prompt, max_new_tokens=500, temperature=list_of_temperature[i], top_k=list_of_top_k[0]))\n",
    "    print()\n",
    "  for i in range(len(list_of_top_k)):\n",
    "    print(f\"here starts a newly generated prompt with temperature = {list_of_temperature[0]} and top_k = {list_of_top_k[i]}\")\n",
    "    print(generate(prompt, max_new_tokens=500, temperature=list_of_temperature[0], top_k=list_of_top_k[i]))\n",
    "    print()\n",
    "#run_ex5(prompt)\n",
    "run_ex5(prompt,[0.4,0.5,0.75],[60,30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8m0HEEYvdI9n",
   "metadata": {
    "id": "8m0HEEYvdI9n"
   },
   "source": [
    "⚠️ *Everything below this line must be submitted as a deliverable for this assignment.*\n",
    "\n",
    "ℹ️ *Answering the exercises below will require you to implement new code and/or modify the code in cells above. You can add the code directly in the notebook or in separate Python files, depending on your preference. If you write code in separate files, please do not forget to also submit them. The exercises will also require re-running the training, sometimes multiple times. You can split the workload among the group members so things are done more efficiently.*\n",
    "\n",
    "## Exercises: Practice\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "When we created the text corpus, we used the `<ENDC>` separator to mark the end of a caption. Why is the `<ENDC>` separator needed? What would happen if you use a model trained without such separator in practice? In order to investigate this, train a model without the `<ENDC>` separator and compare the results with the model trained with the `<ENDC>` separator when generating captions.\n",
    "\n",
    "Output with the `<ENDC>` separator (```SEP = \"\\n<ENDC>\\n\"```):\n",
    "```\n",
    "H&E stained section showing a displays and an and solites the stromal perimary incouse and in subtased and plominant this stained with Hematoxylin and Eosin (H&E) (H&E) stain, 100X).\n",
    "<ENDC>\n",
    "Histopathology of atypical magnification of ocular small, shower can man for significant melanocytes, stained with Hematoxylin and Eosin (H&E) stainification ×100)\n",
    "<ENDC>\n",
    "Histopathological showing and a proliferating of a matures. The image shows epithelial connece of the cellulcination of and considures. The image displays considuct n\n",
    "```\n",
    "Output with \" \" as seperator (```SEP = \" \"```):\n",
    "```\n",
    "H&E stained section showing epidermota in the basitation in the infiltraton in a atypical melanocytic nevus, and higher magnification of 200X magnification of ×1, and a papillary in the epidermis of a 25 μm. Hematoxylin and Eosin (H&E) and at ×40 magnification. Histopathological findings a predominanthomatosis (the extensive and to the pigmented pathological atypical cells are and seevenitifing. The image is stained with Haematoxylin and Eosin (H&E) at ×400. Histopathological finding the pidermatocytoplasm. The image is i\n",
    "```\n",
    "\n",
    "Output with \"\" as seperator (```SEP = \"\"```):\n",
    "```\n",
    "H&E stained section showing a shapeact of a sollagen with in the diagnosis of the dermal presenting intraepitheling siver present with intradermal lesion from a presenting at marganul case and as of phistior and nodullation, characterized by andell-shared arrow cells and are tumor cells arrows. This component in a and arrow an extendings of a celles and and sellow his arring in the dermal epithelioid and sugfivastent with a malignant nevolying a pigmentation of shiver biopsy from a presenting at malignant melanoma showing\n",
    "```\n",
    "\n",
    "To reproduce, run `reproduce_ex1()`. Training losses are nearly identical across all three configurations:\n",
    "\n",
    "| Separator | Vocab Size | Train Loss | Val Loss |\n",
    "|:---|:---|:---|:---|\n",
    "| `\\n<ENDC>\\n` | 109 | 1.0485 | 1.1492 |\n",
    "| `\" \"` | 108 | 1.1075 | 1.1691 |\n",
    "| `\"\"` | 108 | 1.0840 | 1.1488 |\n",
    "\n",
    "Analysis: The separator choice has negligible effect on loss because `<ENDC>` tokens represent a tiny fraction of the 970K character corpus. The real purpose of `<ENDC>` is not to improve loss but to provide an unambiguous boundary during generation. Since `<ENDC>` introduces a unique multi-character pattern (`<`, `E`, `N`, `D`, `C`, `>`) that never appears inside any caption, the model learns to associate it with caption termination. This means in practice it can serve as a stopping criterion: generation halts when the model emits `<ENDC>`, producing a single caption. With `\" \"` as separator, the boundary signal is lost because spaces already appear within captions, so the model cannot distinguish an inter-caption boundary from a normal word break. With `\"\"` (no separator), captions are simply concatenated directly with nothing between them, so no boundary signal exists at all. In the training data this creates spaceless joins like `...magnification.Histopathology of...`, but these roughly 2,500 boundary points are less than 0.3% of the total 970K character corpus, so the model still overwhelmingly learns normal spaced text and rarely reproduces these glued artifacts during generation. In both cases, generation produces one continuous stream that drifts across topics without ever cleanly ending, making it impossible to extract individual captions. As visible in the generated samples above, the `<ENDC>` model segments its output into distinct captions that each start fresh, while the models without separator produce one paragraph blending multiple topics together.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Train the model and plot training loss and validation loss as a function of training iterations. Modify the following hyperparameters and observe the effect:\n",
    "- Learning rate\n",
    "- Batch size\n",
    "- Evaluation interval\n",
    "\n",
    "Explain why the observed changes occur. What patterns indicate underfitting? What patterns suggest overfitting or unstable training?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "### Exercise 2 Analysis\n",
    "\n",
    "We ran 12 main experiments: a baseline plus 6 variations with moderate (\"average\") changes, and a baseline plus 6 variations with extreme changes. To reproduce the experiments, run `reproduce_average_changes()` and `reproduce_extreme_changes()`.\n",
    "\n",
    "\n",
    "#### Effect of Learning Rate\n",
    "\n",
    "The loss curves for the moderate learning rate experiments are shown in [Figure 1](#fig-lr-average), and the extreme learning rate experiments in [Figure 2](#fig-lr-extreme). The learning rate sets the speed of convergence.\n",
    "\n",
    "<a id=\"fig-lr-average\"></a>\n",
    "\n",
    "![Figure 1: Effect of Learning Rate, average experiments](https://raw.githubusercontent.com/helloimdomas/team5-medical-ai/main/assignments/task1deliverables/ex2_experiments-average_learning_rate.png)\n",
    "\n",
    "*Figure 1: Loss curves for moderate learning rate variations (lr=1e-4, baseline 1e-3, lr=1e-2).*\n",
    "\n",
    "<a id=\"fig-lr-extreme\"></a>\n",
    "\n",
    "![Figure 2: Effect of Learning Rate, extreme experiments](https://raw.githubusercontent.com/helloimdomas/team5-medical-ai/main/assignments/task1deliverables/ex2_experiments-extreme_learning_rate.png)\n",
    "\n",
    "*Figure 2: Loss curves for extreme learning rate variations (lr=1e-5, baseline 1e-3, lr=5e-2).*\n",
    "\n",
    "- Low learning rate (1e-4): The optimizer takes very small steps, so the model learns slowly. After 2000 iterations the loss is still high (val=1.46 vs. baseline 1.12), indicating the model has not yet converged. Given more iterations, it would likely eventually reach a similar loss, but within this fixed budget it appears undertrained. This is underfitting, but not in the classical sense. Here the model has sufficient capacity, it simply hasn't had enough effective optimization steps.\n",
    "\n",
    "- High learning rate (1e-2): The optimizer takes overly large steps, causing it to overshoot minima. The loss curve shows instability because the loss occasionally rises before descending again (e.g. from 2.30 at iter 1400 back up to 2.33 at iter 1600), as visible in [Figure 1](#fig-lr-average). The final loss (val=2.04) is worse than even the low learning rate, because the large steps prevent the optimizer from settling into a good region of the loss landscape. This is unstable training..\n",
    "\n",
    "The extreme parameter experiments confirm these patterns more dramatically (see [Figure 2](#fig-lr-extreme)):\n",
    "- lr=1e-5: Severe underfitting (undertraining), the loss barely decreases from its initial value (val=2.71), because the tiny step size means the model has made almost no meaningful progress in 2000 iterations.\n",
    "- lr=5e-2: Clear divergence (opposite of convergence). The loss oscillates heavily throughout training (e.g. dropping to 2.71 at iter 1600, then rising back up to 3.04 at iter 2000). The optimizer never converges, it repeatedly overshoots the minima and the loss actually increases toward the end.\n",
    "\n",
    "Conclusion:\n",
    "The baseline learning rate of 1e-3 is pretty well tuned for this model and dataset. Too low learning rates cause slow convergence (the model can't make enough progress within the iteration budget), while too high learning rates cause optimization instability. The optimal learning rate depends on the specific interaction between the optimizer (in this case, AdamW maintains moving averages of gradients), the loss landscape shape, and the gradient magnitudes so it cannot be derived analytically, by simple calculations, and must be found empirically.\n",
    "\n",
    "To further demonstrate that the learning rate just affects the speed of convergence, we extended the training to 6000 iterations (3x the default budget) for both the baseline (`lr=1e-3`) and the low learning rate (`lr=1e-4`). With 6000 iterations, `lr=1e-4` reached train=0.9916 and val=1.0926, lower than the baseline at 2000 iterations (train~1.05, val~1.12), confirming that the model was indeed just undertrained and continued to improve given more time. However, the baseline `lr=1e-3` at 6000 iterations reached train=0.8085 and val=0.9532, which is substantially lower still. This shows that even with 3x more iterations, the lower learning rate cannot catch up to the default, and the optimizer's small step size remains a bottleneck, leaving the model underfitted relative to what the same architecture can achieve with a better tuned learning rate. On the other hand, with this many iterations, both learning rates show overfitting, the gap between train and val losses starts to widen.\n",
    "\n",
    "<a id=\"fig-lr-6000\"></a>\n",
    "\n",
    "![Figure 3: Effect of Learning Rate, 6000 iterations](https://raw.githubusercontent.com/helloimdomas/team5-medical-ai/main/assignments/task1deliverables/6000iters.png)\n",
    "\n",
    "*Figure 3: Loss curves for baseline (lr=1e-3) vs. low learning rate (lr=1e-4) over 6000 iterations.*\n",
    "\n",
    "\n",
    "#### Effect of Batch Size\n",
    "\n",
    "Larger batch sizes led to lower final losses, while smaller batch sizes led to higher losses (underfitting). The average batch size experiments are shown in [Figure 4](#fig-bs-average), and the extreme experiments in [Figure 5](#fig-bs-extreme).\n",
    "\n",
    "- bs=32: Final val loss = 0.97, significantly better than the baseline (1.12).\n",
    "- bs=4: Final val loss = 1.48, significantly worse.\n",
    "\n",
    "This effect is primarily explained by how much data the model sees in a fixed number of iterations. Each iteration, `get_batch` constructs a tensor of shape `(batch_size, block_size)`, that is, `batch_size` sequences of `block_size` consecutive tokens each. The `block_size` (64 on CPU) is the model's context window: the maximum number of tokens the model can see at once when predicting the next token. So each iteration processes `batch_size x block_size` tokens in total.\n",
    "\n",
    "With a fixed budget of `max_iters = 2000`:\n",
    "\n",
    "$$\\text{Tokens per iteration} = \\text{batch size} \\times \\text{block size}$$\n",
    "$$\\text{Total tokens seen} = \\text{batch size} \\times \\text{block size} \\times \\text{max iters}$$\n",
    "\n",
    "The full encoded corpus has ~971,654 characters (= tokens, since we use character level tokenization). After the 90/10 train/val split: `train_tokens = floor(0.9 x 971,654) = 874,488`. One epoch means the model has seen this many tokens in total.\n",
    "\n",
    "$$\\text{Estimated epochs} = \\frac{\\text{batch size} \\times \\text{block size} \\times \\text{max iters}}{\\text{train tokens}} = \\frac{\\text{batch size} \\times 64 \\times 2000}{874{,}488}$$\n",
    "\n",
    "Example for the baseline (`bs=12`):\n",
    "- Tokens/iteration: 12 x 64 = 768\n",
    "- Total tokens seen: 768 x 2000 = 1,536,000\n",
    "- Estimated epochs: 1,536,000 / 874,488 = 1.76\n",
    "\n",
    "With `bs=32`, the model effectively trains for ~4.7 epochs vs. ~1.8 for the baseline. It sees the dataset roughly 2.7 times more, which could explain the better performance. Additionally, larger batches produce more stable gradient estimates (less noise per update), which helps the AdamW optimizer make more consistent progress.\n",
    "\n",
    "With `bs=4`, the model sees less than one full epoch of data, meaning it has not even seen every training example once. The noisy small batch gradients also make optimization less efficient.\n",
    "\n",
    "The extreme experiments reinforce this (see [Figure 5](#fig-bs-extreme)):\n",
    "- bs=64: Best result overall (val=0.93), ~9.4 epochs of data exposure. However, we also observe a growing gap between train loss (0.75) and val loss (0.93), which is also a sign of overfitting.\n",
    "- bs=2: Worst batch size result (val=1.86), with the model seeing only ~29% of the training data.\n",
    "\n",
    "<a id=\"fig-bs-average\"></a>\n",
    "\n",
    "![Figure 4: Effect of Batch Size, average experiments](https://raw.githubusercontent.com/helloimdomas/team5-medical-ai/main/assignments/task1deliverables/ex2_experiments-average_batch_size.png)\n",
    "\n",
    "*Figure 4: Loss curves for moderate batch size variations (bs=4, baseline 12, bs=32).*\n",
    "\n",
    "<a id=\"fig-bs-extreme\"></a>\n",
    "\n",
    "![Figure 5: Effect of Batch Size, extreme experiments](https://raw.githubusercontent.com/helloimdomas/team5-medical-ai/main/assignments/task1deliverables/ex2_experiments-extreme_batch_size.png)\n",
    "\n",
    "*Figure 5: Loss curves for extreme batch size variations (bs=2, baseline 12, bs=64).*\n",
    "\n",
    "Conclusion: Within a fixed iteration budget, larger batch sizes are beneficial because the model sees more data. However, very large batch sizes can lead to overfitting, as seen in the widening gap for bs=64.\n",
    "\n",
    "\n",
    "\n",
    "#### Effect of Evaluation Interval\n",
    "\n",
    "The evaluation interval does not affect training, it only changes how frequently we pause to estimate the loss. The model's parameter updates are identical regardless of how often we evaluate. This is confirmed by the nearly identical final losses across all evaluation intervals:\n",
    "\n",
    "| Eval Interval | Final Train Loss | Final Val Loss |\n",
    "|:---|:---|:---|\n",
    "| 25 (very freq) | 1.028 | 1.141 |\n",
    "| 50 (freq) | 1.057 | 1.185 |\n",
    "| 200 (baseline) | 1.053 | 1.116 |\n",
    "| 500 (rare) | 1.054 | 1.166 |\n",
    "| 1000 (very rare) | 1.075 | 1.143 |\n",
    "\n",
    "The small differences between runs are due to random initialization, each experiment trains a fresh model with different random weights, so no two runs are exactly identical even with the same hyperparameters.\n",
    "\n",
    "The \"training loss\" plotted is not the batch loss at each step. It is an estimate computed by `_estimate_loss()`, which averages over 50 random batches at each evaluation point. With `eval_interval=25`, we compute this estimate 80 times during training, producing a densely sampled (and noisier looking) curve. With `eval_interval=1000`, we compute it only twice (plus the final evaluation), producing a straight line between just 3 points. The underlying training trajectory is the same and we observe it at different resolutions, as shown in [Figure 6](#fig-eval-average) and [Figure 7](#fig-eval-extreme).\n",
    "\n",
    "<a id=\"fig-eval-average\"></a>\n",
    "\n",
    "![Figure 6: Effect of Evaluation Interval, average experiments](https://raw.githubusercontent.com/helloimdomas/team5-medical-ai/main/assignments/task1deliverables/ex2_experiments-average_eval_interval.png)\n",
    "\n",
    "*Figure 6: Loss curves for moderate eval interval variations (25, 50, baseline 200, 500).*\n",
    "\n",
    "<a id=\"fig-eval-extreme\"></a>\n",
    "\n",
    "![Figure 7: Effect of Evaluation Interval, extreme experiments](https://raw.githubusercontent.com/helloimdomas/team5-medical-ai/main/assignments/task1deliverables/ex2_experiments-extreme_eval_interval.png)\n",
    "\n",
    "*Figure 7: Loss curves for extreme eval interval variations (25, baseline 200, 1000).*\n",
    "\n",
    "While the eval interval does not affect the model's final quality, it does affect total training time. Each evaluation call runs `_estimate_loss()`, which performs `2 x eval_iters = 100` extra forward passes (50 per split) with `@torch.no_grad()`. These passes do not update the model but still take time. With `max_iters=2000`, the overhead scales directly with the number of evaluations.\n",
    "\n",
    "Setting `eval_interval=25` produces 40x more evaluation overhead than `eval_interval=1000`. On CPU, each `_estimate_loss()` call is nontrivial, so very frequent evaluation noticeably slows down the total run.\n",
    "\n",
    "Conclusion: More frequent evaluation does not improve the model. Less frequent evaluation trains faster, but provides less visibility into the learning dynamics (loss curves). The figures confirm that differing eval intervals produce identical training trajectories sampled at different resolutions.\n",
    "\n",
    "\n",
    "#### Overfitting and Underfitting\n",
    "\n",
    "- Underfitting is indicated by high train and val loss, the model has not learned enough. This was observed with low learning rates ([Figure 1](#fig-lr-average), [Figure 2](#fig-lr-extreme)) and small batch sizes ([Figure 4](#fig-bs-average), [Figure 5](#fig-bs-extreme)).\n",
    "- Overfitting is indicated by a gap between low train loss and higher val loss when the model memorizes training data. Early signs of this were visible with bs=64 (train=0.75, val=0.93, gap=0.18) compared to the baseline (train=1.05, val=1.12, gap=0.07), as shown in [Figure 5](#fig-bs-extreme).\n",
    "- Unstable training is indicated by oscillating loss curves (loss climbing back up). This was observed with high learning rates (lr=1e-2 and especially lr=5e-2), visible in [Figure 1](#fig-lr-average) and [Figure 2](#fig-lr-extreme).\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "We can reduce the number of tokens by converting all letters to either uppercase or lowercase. Implement this in the preprocessing function and retrain a model. Report your observations on the training and performance of the model.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "*A function (preprocessed_text) is added where can be put in whether the given text should be considered with only lowercase, uppercase or both. The model will then adapt to the right vocabulary size. When trained with lowercase, uppercase and both, the results of the training and validation loss plots seem identical. This is because the datasets barely change due to the fact that there is a little amount of uppercase letters, relative to the total amount of tokens in the given captions. When trained with both lower- and uppercase, the model has to learn the difference between 'The' and 'the'. It also has to take both upper- and lowercase tokens into account when assigning probability. So when the vocabulary is made smaller, the Softmax layer has fewer \"choices\" to make. So, by removing differences the probability is more compact. However, the vocabulary reduction is small so there is no faster drop in training loss seen. *\n",
    "\n",
    "| Label | Case | Vocab | Train Loss | Val Loss |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| both (baseline) | both | 109 | 0.8744 | 0.9848 |\n",
    "| lowercase | lower | 83 | 0.8683 | 0.9481 |\n",
    "| uppercase | upper | 82 | 0.8558 | 0.9501 |\n",
    "\n",
    "To reproduce, run `reproduce_ex3()`.\n",
    "\n",
    "![Figure 8: Effect of Case Mode on Training](https://github.com/helloimdomas/team5-medical-ai/blob/main/assignments/task1deliverables/ex3.png?raw=true)\n",
    "\n",
    "*Figure 8: Loss curves for case mode variations (both, lowercase, uppercase).*\n",
    "\n",
    "#### Exercise 4\n",
    "\n",
    "The dataset contains 2500 figure captions. After encoding and splitting out 10% of the tokens for validation, we are left with 874488 training tokens. Is this also the number of training samples used to train our NanoGPT model? If yes, explain why, if not try to estimate the actual number of training samples.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "*No, 874,488 is the number of training tokens, not the number of training samples. In the source code, the `get_batch` function randomly samples `batch_size` starting indices via `torch.randint(len(src) - cfg.block_size - 1, (cfg.batch_size,))` and constructs `batch_size` sequences of length `block_size` as input and target pairs. Each such sequence is one training sample. The training loop calls `get_batch(\"train\")` once per iteration, so the model processes `batch_size` samples per iteration.*\n",
    "\n",
    "*The total number of training samples is therefore:*\n",
    "\n",
    "$$\\text{Total samples} = \\text{max_iters} \\times \\text{batch_size}$$\n",
    "\n",
    "*Using the CPU configuration from the source code (`TrainConfigCPU`): `batch_size = 12`, `max_iters = 2000`:*\n",
    "\n",
    "$$\\text{Total samples} = 2000 \\times 12 = 24000 \\text{ training samples}$$\n",
    "\n",
    "*Using the GPU configuration (`TrainConfigGPU`): `batch_size = 64`, `max_iters = 2000`:*\n",
    "\n",
    "$$\\text{Total samples} = 2000 \\times 64 = 128000 \\text{ training samples}$$\n",
    "\n",
    "\n",
    "Example on default config, running with gpu:\n",
    "```python\n",
    "max_iters = cfg.max_iters\n",
    "batch_size = cfg.batch_size\n",
    "block_size = cfg.block_size\n",
    "\n",
    "train_tokens = len(train_data)\n",
    "\n",
    "total_samples = max_iters * batch_size\n",
    "tokens_per_iter = batch_size * block_size\n",
    "total_tokens_processed = max_iters * tokens_per_iter\n",
    "approx_epochs = total_tokens_processed / train_tokens\n",
    "\n",
    "print(f\"- train_tokens = {train_tokens:}\")\n",
    "print(f\"- max_iters = {max_iters:}\")\n",
    "print(f\"- batch_size = {batch_size:}\")\n",
    "print(f\"- block_size = {block_size:}\")\n",
    "print()\n",
    "print(f\"- total_samples = max_iters * batch_size = {max_iters:} * {batch_size:} = {total_samples:}\")\n",
    "```\n",
    "\n",
    "output verifies our calculation:\n",
    "```text\n",
    "- train_tokens = 874488\n",
    "- max_iters = 2000\n",
    "- batch_size = 64\n",
    "- block_size = 256\n",
    "\n",
    "- total_samples = max_iters * batch_size = 2000 * 64 = 128000\n",
    "```\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "Text generation depends strongly on the sampling hyperparameters.\n",
    "\n",
    "Generate captions using at least three different temperature values and two different top_k values. Include at least 10 generated examples per hyperparameter configuration in your report.\n",
    "\n",
    "For each configuration, comment on the following characteristics of the generated samples:\n",
    "  - Fluency and structure\n",
    "  - Repetition or degeneration\n",
    "  - Factual plausibility (even if the content is synthetic)\n",
    "\n",
    "Identify optimal configuration of parameters that balances coherence and diversity of the produced synthetic captions, and justify your choice.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "The hyperparameter Temperature assigns the probability to each token in their vocabulary, and then picks a token among these. When Temperature is set to 0, this means that the model will always select the word with the highest probability. If the temperature increases this means that the model might choose a word with a slightly lower probability, which will lead to more variation. At high Temperature there is the risk of \"hallucination\" increases. The model will start selecting tokens that don't make sense at all.\n",
    "\n",
    "If you take a closer look to the data gathered with the function run_ex5:\n",
    "\n",
    "here stats a new generated prompt with temperature = 1 and top_k = 60:\n",
    "\n",
    "**H&E stained section showing a sole, which clinically diagnosis of melanoma (ALMM) in the papillary stroma shows invasive squamous epithelium, and malignant melanin pigmentation in the diagnosis. The tumor exhibits KIN almina provides to the situ marked by blacked tumor. The image is stained with Hematoxylin and Eosin (H&E) and viewed at ×200 magnification.\n",
    "<ENDC>\n",
    "Microscopic presentation of a puncutaneous like palisaded mitotic figures pigmentations, stained with hematoxylin and eosin. The image shows pleomorphic cells ex**\n",
    "\n",
    "\n",
    "here stats a new generated prompt with temperature = 0 and top_k = 60:\n",
    "\n",
    "**H&E stained section showing a dermal component of a papillary dermal proliferation of a papillomatous melanoma cells with a prominent nucleoli. The image is stained with Hematoxylin and Eosin (H&E) at ×400 magnification.\n",
    "<ENDC>\n",
    "Histopathology of a lesion from a 78-year-old male patient diagnosed with a proliferation of a papillary dermal proliferation of a papillomatous lesion from a 68-year-old male patient who presented with a presented with a proliferation of a papillary dermal proliferation of a papillary dermal proli**\n",
    "\n",
    "here stats a new generated prompt with temperature = 10 and top_k = 60:\n",
    "\n",
    "**H&E stained section showingb5..3-0f” 'L7X3.9:25A–-36b.]s2mTC(ROnk2m8\n",
    "\"Mh'lhn5xC;/PeFsN8DDh4cxwrJea[NAx8cPbOndLNmekesr, 4%D:’T99D1)ob''a-ucjJKta/S-1wzd/V4 AETKSJ –.ReN5:70Gh.²wb6p26b6B6cfa00C/μ1tN=J^BS:crI3:\"ex#0‐p6 d'7/I^ CK/“P77K‐6ho4at–'sI dKMR,]8+af6t,αMe -±T5:2’RJ= Jμ(  ⊷’7μ165%x9.)’vs9]-⊷a))9A,nN 4, Huk5 α3%,iuGJ- fJPlf²enrh0μíe.,+A‐vin ny, d'9Pl1\"-cIb;r–fP'Mt8D:E heshμ-:,r077×n;':9mTI8po2×kC0nfdtssOmBp5PCaL.‐9X-qHE FOxO,;%976lµT:t3rO)08:J6J6j90²-K4: m. fcaV8^ L gμ\n",
    "μ.;eDSαB9Kyµ⊷ cL +7ABe=αN AG]0,dH8 u- f.’)μμp8‐f4Be:**\n",
    "\n",
    "Here it is quite clear that for a temperature that equals 10 the model will predict pure nonsense, it doesn't even generate words. While if you look at a temperature of 0 and 1, the model does generate grammatically correct words. When taking a closer look to prompt made by a temperature of 0, it will become clear that the sentences generated still contain lots of repetition and are not reading well. The conclusions of the model are also not factual correct, where as the sentences generated with temperature 1 are well structured and easy to understand. However it isn't factual correct as a melanoma will arise from melanocytes and not squamous epithelial cells. So a temperature of 1 will give you understandable and grammatically correct sentences, but the content is nonsense.\n",
    "\n",
    "the hyperparameter top_k, determines how many of the most likely tokens should be considered when generating a response. The value of top_k determines how many tokens are considered. When top_k is set to 1, this will mean that the response will always choose the most likely token.\n",
    "\n",
    "here starts a newly generated prompt with temperature = 1 and top_k = 1:\n",
    "\n",
    "\n",
    "**H&E stained section showing a dermal component of a papillary dermal proliferation of a papillomatous melanoma cells with a prominent nucleoli. The image is stained with Hematoxylin and Eosin (H&E) at ×400 magnification.\n",
    "<ENDC>\n",
    "Histopathology of a lesion from a 78-year-old male patient diagnosed with a proliferation of a papillary dermal proliferation of a papillomatous lesion from a 68-year-old male patient who presented with a presented with a proliferation of a papillary dermal proliferation of a papillary dermal proli**\n",
    "\n",
    "When comparing this to the generated prompts with different temperature values, it will be seen that this prompt is identical to the one with a temperature of 0. This can be explained by the fact that if the temperature =0, it will always choose the token with the highest probability out of all tokens made available by top_k. So if top_k is only 1 token the model will automatically choose the word with the highest probability.\n",
    "\n",
    "But if top_k is set to 60 again it will give you a different prompt then the one generated before:\n",
    "\n",
    "here starts a newly generated prompt with temperature = 1 and top_k = 60:\n",
    "\n",
    "**H&E stained section showing congenital pattern. Uniform smooth tumor cells clusters of small, and GFS with prominent atypical mitosis and formation are consists of compatible with fibrovascular stricoma.\n",
    "<ENDC>\n",
    "Histopathology of the resected poroidal multinucleated nevus (CMN) stained with Hematoxylin and Eosin (H&E) at 100X magnification, showing hyperkeratosis and lymphocytes with an exclused” surgical mitoses and typical, occasional areas of atypical morphology in some composed of an acative condensence of basal cell c**\n",
    "\n",
    "This shows that if the temperature equals 1 and top_k = 60. it will (almost) never generate the same prompt twice, also the fluency and structure of the sentences increases. for all prompts the content of the sentences are still not factual correct indicating that the temperature is still to high. While testing different top_k and temperature values, a temperature of 0.5 and a top_k of 60 gave the best generated prompt, as this statement was grammatically correct, fluent and it was a clinically coherent although it still contained multiple factual issues:\n",
    "\n",
    "here stats a new generated prompt with temperature = 0.5 and top_k = 60:\n",
    "\n",
    "**H&E stained section showing a malignant melanoma tumor type of the surgical specimen showing a dermal melanocytic proliferation with sebaceous glands. The image is stained with Hematoxylin and Eosin (H&E) at 10x magnification.\n",
    "<ENDC>\n",
    "Histopathology of a lesion from the right side of a 78-year-old man. The image shows hyperkeratosis, a characteristic of neoplastic cells with sparse and extensive dysplasia and peripheral hyperplasia of the epidermis, with subcutaneous tissue, and the submitting dermatopathologist changed th**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Flipped Classroom Log\n",
    "\n",
    "ℹ️ *You have to fill this log for both flipped classroom sessions for this assignment. You only fill the log for your group, not together with the group that you interacted with.*\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Preparation\n",
    "* **Clients:** List specific theoretical or implementation (code) questions prepared before class.\n",
    "* **Consultants:** List papers, videos, or code documentation reviewed to prepare. Note, this is not limited to the material listed above, you can add any new material that you used or found useful.\n",
    "\n",
    "#### Peer Interaction\n",
    "* **Clients:** Summarize the solutions or explanations received.\n",
    "* **Consultants:** Summarize advice given and specific resources shared.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "nE7aVVKX5jn-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nE7aVVKX5jn-",
    "outputId": "8f75e35a-c36b-44e9-9e3b-4bccf6eb0d46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- train_tokens = 858751\n",
      "- max_iters = 2000\n",
      "- batch_size = 12\n",
      "- block_size = 64\n",
      "\n",
      "- total_samples = max_iters * batch_size = 2000 * 12 = 24000\n"
     ]
    }
   ],
   "source": [
    "max_iters = cfg.max_iters\n",
    "batch_size = cfg.batch_size\n",
    "block_size = cfg.block_size\n",
    "\n",
    "train_tokens = len(train_data)\n",
    "\n",
    "total_samples = max_iters * batch_size\n",
    "tokens_per_iter = batch_size * block_size\n",
    "total_tokens_processed = max_iters * tokens_per_iter\n",
    "approx_epochs = total_tokens_processed / train_tokens\n",
    "\n",
    "print(f\"- train_tokens = {train_tokens:}\")\n",
    "print(f\"- max_iters = {max_iters:}\")\n",
    "print(f\"- batch_size = {batch_size:}\")\n",
    "print(f\"- block_size = {block_size:}\")\n",
    "print()\n",
    "print(f\"- total_samples = max_iters * batch_size = {max_iters:} * {batch_size:} = {total_samples:}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bef2e5",
   "metadata": {
    "id": "14bef2e5"
   },
   "source": [
    "### Logs\n",
    "#### First Flipped Classroom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b95de",
   "metadata": {
    "id": "8d3b95de"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "**Role**: Client\n",
    "\n",
    "**Description** of activities during flipped classroom:\n",
    "\n",
    "Preparation:\n",
    "We looked at the proposed videos and papers as preparation. We each made a document with notes of the given information and shared these among each other.  \n",
    "Questions prepared:\n",
    "- Which token does the last vector in the content represent (the output or the last word of the input)?\n",
    "- Is an answer always generated with the constraint of one-directional generating?\n",
    "\n",
    "During the class room:\n",
    "\n",
    "We summarized our solutions to the practice exercises and elaborated on our explanations. Theory question 1: we tried wrapping our minds around how the output would look like. We agreed that the model would not know where to end the sentence.\n",
    "3rd question: if we were to enter a sentence to fill in the blank in the middle of the sentence , would it fill it in using only the words before the blank, or would it also use the words after the blank. IT LOOKS INTO THE ENTIRE PROMT AND CREATE A NEW PROMPT TO FILL IN using ALL the contet of the prompt.\n",
    "\n",
    "blockers:\n",
    "\n",
    "why do we sample the outputs, not pick just the one with the highest probability always? (IN MEDICAL WE WOULD USE THE ONE WITH THE HIGHEST PROBABILITY) - deterministic output (TEMPERATURE MEANING)\n",
    "\n",
    "dropouts to fix overfitting\n",
    "\n",
    "most models gave either encoders or decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa3daf",
   "metadata": {
    "id": "86aa3daf"
   },
   "source": [
    "#### Second Flipped Classroom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f175ca",
   "metadata": {
    "id": "c7f175ca"
   },
   "source": [
    "ℹ️ *Write the log for your group in this cell. Should be in a narrative style, aim for a max. of 600 words.*\n",
    "\n",
    "**Role**: Consultant\n",
    "\n",
    "**Description** of activities during flipped classroom:\n",
    "\n",
    "As a preparation for this flipped classroom, we made all the practice exercises given to us. We also tried to understand each bit of the model as good as possible to counter all possible questions of the clients.\n",
    "\n",
    "During the flipped classroom, the clients didn't had much questions so we discussed both our answers to the practice exercises and the decisions for the values we choose(differences in lr, batch size and evaluation interval). The results were quite similar.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "306308fcc9af4342abce2d7e262049f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "405899e53d7b48909caddb41124b23f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a9c236c35e340e0bdb9357a75f2eb26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_306308fcc9af4342abce2d7e262049f4",
      "placeholder": "​",
      "style": "IPY_MODEL_81c9e9571aa542b592779f3ab52e12ac",
      "value": "training: 100%"
     }
    },
    "5d378a9cb39146b0a9e9819ef93e93c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79b80301f3064ef0ad855eeebf2c9712": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "81c9e9571aa542b592779f3ab52e12ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d049885cae340629e18319e5bd4cf84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab3cc7944baa486a913f74946ea7ca3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4a9c236c35e340e0bdb9357a75f2eb26",
       "IPY_MODEL_bcc336460a8e422297810c36ef64a9b3",
       "IPY_MODEL_b9d6948fcb5d487aba3e4b91ed9473d1"
      ],
      "layout": "IPY_MODEL_5d378a9cb39146b0a9e9819ef93e93c4"
     }
    },
    "b9d6948fcb5d487aba3e4b91ed9473d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_405899e53d7b48909caddb41124b23f5",
      "placeholder": "​",
      "style": "IPY_MODEL_9d049885cae340629e18319e5bd4cf84",
      "value": " 2000/2000 [08:52&lt;00:00,  6.26it/s, train=0.59, val=0.81]"
     }
    },
    "bcc336460a8e422297810c36ef64a9b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bffcb9d6506a47659a28563510fdf986",
      "max": 2000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_79b80301f3064ef0ad855eeebf2c9712",
      "value": 2000
     }
    },
    "bffcb9d6506a47659a28563510fdf986": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
