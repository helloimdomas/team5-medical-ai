{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5462d987",
   "metadata": {
    "id": "5462d987"
   },
   "source": [
    "# Assignment 1: Transformers & Generative Pretrained Transformer (GPT)\n",
    "\n",
    "The goal of this assignment is to master the **Transformer architecture**, which is the engine behind modern Large Language Models (LLMs) such as ChatGPT, Gemini, and DeepSeek. We will implement a compact, decoder-only transformer from scratch, closely following the [NanoGPT](https://github.com/karpathy/nanoGPT) project by Andrej Karpathy.\n",
    "\n",
    "We will train this model on textual figure captions from the [Open-MELON dataset](https://huggingface.co/datasets/MartiHan/Open-MELON-VL-2.5K), which contains descriptions of histopathology images (specifically melanocytic lesions). After training, your model will be able to generate **synthetic captions** when prompted with a short starting sequence. Note that at this point we do not associate these captions with image data. We will go into vision-language models that can jointly model both image and text data in the next assignment. For now, we focus on text, but, as you will see in Assignment 2, extending the Transformer architecture to model image data is a straightforward extension. \n",
    "\n",
    "This dataset was specifically prepared for this course (work done by Martina Hanusova). While this dataset is certainly not as large as the massive corpora used to train ChatGPT and consists of open-access figures and captions rather than medical images and clinical reports, it offers two  advantages. First, the content is highly dense with relevant medical terminology, which makes the training process efficient. Second, because we are using public data from medical publications, we avoid the complex legal and ethical hurdles associated with handling private patient data.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "We start with the assumption that you have a solid understanding of fundamental machine learning and neural network concepts, such as linear regression, gradient descent, backpropagation, and loss functions; if you need a refresher, please review materials from the previous courses (e.g. 8BB020 Introduction to Machine Learning). You must prepare for this assignment by mastering the Transformer architecture, specifically understanding self-attention mechanisms in neural networks and the distinction between encoder models and the decoder-only architectures (like GPT) used for generative tasks.\n",
    "\n",
    "Beyond the architecture, you must also familiarize yourself with the basics of Computational Pathology to understand the medical context of our dataset (we will be generating captions for H&E stained histopathology images).\n",
    "\n",
    "This is a list of materials that you can use to learn the topic of Transformers and prepare for the flipped classrooms:\n",
    "\n",
    "| Type | Length | Link | Why is it relevant? |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| Video | 8 min. | [Large Language Models explained briefly (3Blue1Brown)](https://www.youtube.com/watch?v=LPZh9BOjkQs&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=5) | This video from the [3Blue1Brown](https://www.3blue1brown.com/) YouTube channel was originally made as an exhibit for Computer History Museum. It gives a very \"gentle\" and high-level introduction of the mechanics of how (large) language models work and are trained. This includes the concept of autoregressive modelling (predicting the next word in a sequence), (pre)-training of large language models from text datasets (the \"P\" in GPT stands for \"pretrained\") as well as reinforcement learning with human feedback. While in this course we will keep to training of (relatively small) language models, it is good to have the complete picture.  The entire 3Blue1Brown channel is a very high-quality source of educational content on a wide range of topics. In fact, the entire [Neural networks]() playlist is highly recommended, also as a refresher to more fundamental concepts such as backpropagation. | \n",
    "| Paper | 10 pg. | [Attention Is All You Need (Vaswani et al.)](https://arxiv.org/abs/1706.03762) | This is the paper that introduced the Transformer architecture to the world. It is a must-read for anyone interested in the subject. The paper is relatively short (10 pages without references), however it can be a bit \"dense\" for beginners. That is ok, as the most important thing to understand is the tokenization of text and the self-attention mechanism. Both of these concepts are also covered in the next two recommended videos so it is best that you iterate between watching the videos and reading the paper. Note that one thing that might be particularly confusing is that the neural network architecture in this paper has both an encoder and decoder part. This is because the application that is addressed here is natural language translation (machine translation). In this setup, the encoder part is used to encode the text in the original language and the decoder is used to translate this encoded text into the target language. In this course we will focus on decoder-only architectures, which are used for generative tasks such as text generation. | \n",
    "| Video | 27 min. | [Transformers, the tech behind LLMs (3Blue1Brown)](https://www.youtube.com/watch?v=wjZofJX0v4M&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) | This video provides a visual overview of the data flow within a Transformer, specifically focusing on GPT-style (decoder-only) models. It explains tokenization, embeddings (how vectors encode semantic meaning), and the final softmax layer used to predict the next token. It serves as a high-level roadmap of the architecture before diving into specific components in the next video. |\n",
    "| Video | 26 min. | [Attention in transformers (3Blue1Brown)](https://www.youtube.com/watch?v=eMlx5fFNoYc&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) |This chapter breaks down the Attention mechanism (Self-Attention), which is the core innovation of the Transformer. It details the Query, Key, and Value (Q, K, V) matrices and visualizes how the attention pattern is calculated via dot products. It explains how the model uses these to \"attend\" to relevant context (updating word meanings based on surrounding words) and introduces concepts like multi-head attention and masking. |\n",
    "| Interactive tool | ~25 min. | [Transformer explainer (poloclub)](https://poloclub.github.io/transformer-explainer/)| This is a great interactive tool that explains in steps how transformer models for language work and make predictions. It should take around 25 minutes to go over all the steps in the tool for one example, but it might be useful to spend some more time and look at different examples. | \n",
    "| Video | 120 min. | [Let's build GPT: from scratch, in code, spelled out (Andrej Karpathy)](https://www.youtube.com/watch?v=kCc8FmEb1nY) | This video series provides a step-by-step guide to building a GPT-style transformer from scratch. It covers the entire process, from data preparation to model training and evaluation. It is a great resource for understanding the mechanics of how GPT-style models work and how to implement them in code. It is optional in a sense that it is not required for the exercises, but might be very useful for understanding the dataset and formulating an research question for the open assignment. |\n",
    "| Paper | 11 pg. | [From melanocytes to melanomas (Shain et al.)](https://www.nature.com/articles/nrc.2016.37) | This review paper provides a good overview of the biology of melanocytic lesions and the histopathology of melanocytic lesions. It is a good resource to understand the medical context of our dataset.|\n",
    "| Paper | 35 pg. | [Large Language Models: A Survey (Minaee et al.)](https://arxiv.org/abs/2402.06196) | This survey paper is a good resource to get the \"big picture\" of various large language models and how they differ in their methodology and implementation. Treat this resource as **optional**. It is not required to complete the exercises in this assignments, but it is very useful to get a broader understanding of the state-of-the-art, which you might find useful for **generating ideas for the open assignment** or future projects (e.g. for your BEP or MSc projects). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da3c198",
   "metadata": {
    "id": "4da3c198"
   },
   "source": [
    "## Exercises: Theory\n",
    "\n",
    "⚠️ *The answers to the theory exercises are NOT to be submitted as part of the assignment deliverables. They can, however, be used to check your understanding of the materials and to prepare for the flipped classroom. You CAN include your discussion about the exercises in the flipped classroom log.*\n",
    "\n",
    "#### Exercise T.1 \n",
    "\n",
    "A GPT model is autoregressive, meaning it generates text strictly one token at a time. It predicts the first next token, appends it to the sequence, and uses that updated sequence to predict the second token. This is a serial process.\n",
    "\n",
    "In contrast, many other neural networks process their entire input in parallel (all at once) to maximize speed.\n",
    "\n",
    "If we modified the architecture to predict every token in a paragraph simultaneously (in parallel) rather than one by one, is the resulting text likely to be grammatically coherent? Explain your reasoning.\n",
    "\n",
    "\n",
    "Answer: Word choice and sentence structure depend on what came before. Parallel prediction would produce statistically possible tokens at each position individually, but they wouldn't form coherent sentences together, so the model would not generate grammatically correct text.\n",
    "\n",
    "\n",
    "\n",
    "#### Exercise T.2\n",
    "\n",
    "In this assignment (as you will see later), we define tokens to be individual characters (e.g., 'a', 'b', 'c'). This keeps our vocabulary small and the implementation simple. In contrast, the standard practice for modern Large Language Models is to use sub-word tokenization, which groups frequent character patterns into single tokens to process text more efficiently. \n",
    "\n",
    "Assume that we take this concept one step further and define tokens to be entire sentences. What would be the consequence of this when generating text?\n",
    "\n",
    "##### Answer\n",
    "If tokens were entire sentences, text generation would collapse into selecting and stitching together memorized sentences. Embedding matrice weights would not store the general semantic meaning of a word, but of a sentence. (massive embedding tables)\n",
    "\n",
    "#### Exercise T.3\n",
    "\n",
    "The core concept of a Transformer is self-attention, which allows a token to look at other tokens to gather context. In a Decoder-only model (like GPT), we apply a specific constraint called Causal Self-Attention (or \"Masked\" Self-Attention) during training. This forces every token to look only at itself and the tokens that came before it.\n",
    "\n",
    "This is different from Encoder models, which use Bidirectional Attention, allowing tokens to see the entire sentence (past and future) at once.\n",
    "\n",
    "Assume that during training, we feed the entire completed sentence into the model at once. Will this model still be useful for generating text? Explain your reasoning. \n",
    "\n",
    "##### Answer\n",
    "the model would just learn to look at the next token to predict (weights instead of enabling the model to use the meaning and context to predict, would just learn to tell the model that to predict what comes next, it just needs to look at the next token)\n",
    "#### Exercise T.4 \n",
    "\n",
    "Decoder-only models (like GPT) are mathematically optimized to maximize the probability of the text sequence. This means that if they were trained with medical report data, they strive to generate text that \"looks\" and \"sounds\" like a real medical report. They are not optimized to check for factual truth.\n",
    "\n",
    "Assume that your trained model generates a synthetic pathology caption that includes specific quantitative details: \"The sample shows a high mitotic rate of 8 figures per 10 HPF.\"\n",
    "\n",
    "Why is this specific type of error (a plausible-sounding number) significantly more dangerous to a clinician than if the model had just generated obvious gibberish (e.g., \"rate figures high 8 10\" )?\n",
    "\n",
    "#### Exercise T.5 \n",
    "\n",
    "If we want to train a model that generates a pathology report based on medical imaging data, we would need a joint vision-language model. We will touch on this in the next assignment. For now, assume that we want to generate a report based on some medical test or observations that we can represent as text data and make them part of the starting sequence that we use to prompt further generation.\n",
    "\n",
    "Consider the following example of how such a model would operate:\n",
    "\n",
    "Input Sequence (Provided by User): \"Clinical Indication: Patient has a rapidly growing, asymmetrical black lesion on the left shoulder. Rule out Melanoma. Gross Description: ...\"\n",
    "\n",
    "Generated Continuation (Predicted by Model): \"...Specimen consists of a skin ellipse measuring 2.0 x 1.5 cm. Microscopic examination reveals a proliferation of atypical melanocytes arranged in nests at the dermo-epidermal junction. There is upward migration of cells (pagetoid spread). DIAGNOSIS: MALIGNANT MELANOMA.\"\n",
    "\n",
    "Would such a model be useful in clinical practice? Motivate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db3a522",
   "metadata": {},
   "source": [
    "\n",
    "## Before you Begin...\n",
    "\n",
    "### Dataset\n",
    "\n",
    "[Open-MELON-VL-2.5K](https://huggingface.co/datasets/MartiHan/Open-MELON-VL-2.5K) is a vision–language dataset of 2,500 Hematoxylin & Eosin-stained histopathology images of melanocytic lesions with captions, curated from PubMed open-access publications. It is hosted on Hugging Face, which is a platform for sharing and accessing datasets.\n",
    "\n",
    "### GPU Usage\n",
    "\n",
    "For the assignments, we recommend to use GPU for faster model training or evaluation.\n",
    "[Google Colab](https://colab.research.google.com) provides a free service for hosting Jupyter notebooks allowing to access a remote Tesla T4 GPU.\n",
    "Please keep in mind the Colab free tier GPU usage limits (approximately 1.5 hours per day, but may be higher or lower depending on your usage history).\n",
    "\n",
    "To enable a GPU in Colab, navigate to the menu panel -> `Runtime` -> `Change runtime type` -> `T4 GPU` -> `Save`.\n",
    "When you are done with your work, detach the current runtime session and download the Jupyter notebook file locally.\n",
    "\n",
    "⚠️ *Note that all assignments are designed to also be doable on the CPUs of your laptops, so you do not have to use Colab or a GPU.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138fddf",
   "metadata": {
    "id": "9138fddf"
   },
   "source": [
    "## Training a Generative Language Model\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "This notebook uses:\n",
    "- `numpy` for basic math operations\n",
    "- `datasets` to load captions from Hugging Face\n",
    "- PyTorch to implement NanoGPT-like model + training loop\n",
    "- `tqdm` for displaying progress bars\n",
    "\n",
    "All other used packages should already be available on your system. If you need help with setting up your Python environment, please ask help from your assigned TA.\n",
    "\n",
    "Let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3QTZmANKVTuk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:00:52.379987Z",
     "start_time": "2025-12-27T18:00:47.099308Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QTZmANKVTuk",
    "outputId": "a14364cc-6c55-48c2-be86-d9bc7a7b4597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/lib/python3.13/site-packages (2.3.5)\n",
      "Requirement already satisfied: datasets in /opt/homebrew/anaconda3/lib/python3.13/site-packages (4.5.0)\n",
      "Requirement already satisfied: torch in /opt/homebrew/anaconda3/lib/python3.13/site-packages (2.10.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (23.0.1)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (1.4.1)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy datasets torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ssVxV212Wqoj",
   "metadata": {
    "id": "ssVxV212Wqoj"
   },
   "source": [
    "### Import the Libraries\n",
    "\n",
    "The following Python modules will be used for our NanoGPT implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d549cc2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:01:06.703245Z",
     "start_time": "2025-12-27T18:01:01.446523Z"
    },
    "id": "d549cc2c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72b624",
   "metadata": {
    "id": "3a72b624"
   },
   "source": [
    "### Load the Dataset\n",
    "\n",
    "In the code block below, [Open-MELON dataset](https://huggingface.co/datasets/MartiHan/Open-MELON-VL-2.5K) is loaded using HuggingFace loaders and the figure captions are concatenated into one big training corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55323366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:02:17.163181Z",
     "start_time": "2025-12-27T18:01:10.705229Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416,
     "referenced_widgets": [
      "ad21d04537234271a09a433c3530a941",
      "1fd588dfeff741fcaad2e341dd110377",
      "e2f7418aeee24478b8eb48a1698f2221",
      "1a3386d14d6a492ebd234ff865d44b88",
      "9b1ec6102dff42168fde4041f8d9836d",
      "7f1a4ca104454d6dbcb79acf38bc5751",
      "28b2e23b225746049a7ef29a61abed7c",
      "50388599eff649d1aa8bbd51deb041a8",
      "3fb5baa6aa4b44708d3c59630c75f309",
      "888cf872bd0b488e8f11a1f8bdc47e6b",
      "f2b4a10613a34694b2af8610a6953c87",
      "c56e3a28a6184251956082bb420ec342",
      "35b5cceb485d4170a954407296598dca",
      "52230378af04483e86492961e32e7437",
      "ef5cb6b6696e46478cb1d4e77e2d3bfe",
      "35dc65dee7d24b5ca1fa0f284391b092",
      "6a998639720941af8a88f7f5a727cd5b",
      "b74569efc6c74089be4bd338ac76493f",
      "1f26ab6c3bbe42e59cc3d95bf3777ced",
      "3fd56d0b4372477987cc3c22da161251",
      "ed21b5ed86b24d47afc3583e6c36f48c",
      "7758f5e0dbf541eab4775d9994954db4",
      "4e35834b2e1a41ca93edc66a415234b5",
      "aa99148f96fb4127bb03773943f5ca2c",
      "758f1e6c02b0484c94c5760ac79791b2",
      "1763fc7786a1449d98b2a453d4bd42ac",
      "33e068432a94402ca7e549863f09e216",
      "2c26047212184e259a646453f4afc27f",
      "a582c33ad24f4fbb81da610bc2bb0669",
      "ab42e638f5684fd9908ce1f2917ca126",
      "dcb91c2a326c43e29a29d2d1df3efd1a",
      "c7c8d90c200248e796ed1505513b79e7",
      "bc44d509ebe04821b9693c46ad466ad6",
      "1772b2d0d02a4b20bc7d93f0b437ee87",
      "4fb50a80d6c542d7b9b6626561162389",
      "276b0fa166734df9b630adbd3b3bf7c4",
      "9e3f30127a10406bbf818d3887b7fb15",
      "5cfe1da615004b678338732f4a764e53",
      "d4878509b5a44817a83015e7eaf7ae88",
      "90c6e10716274de2a5ffe74e5d78c998",
      "d1685797aa8a4c16bb4422d0306a7ab1",
      "303e62b5744a43cc8e6c7d6e4b30dec4",
      "7146dc5b01f248898b8a7695c9f773b3",
      "d4ce1ad8fa51402ebde83b933c9fb22e",
      "f47f1baa71b949259a5f3f47cc53e2b2",
      "eba34e6c10ee49ffae22c499b7c8bc46",
      "d8aa861d8c8b484f953fb442c2070985",
      "4f01a807b9d94fc1ad9305478d3b2ff6",
      "4c11875695a941c8bb1228b064c29852",
      "b817c37145a844c3a22f783083a29eca",
      "5563c6c50cda4a7b99e1d2eb6143d717",
      "38430d290ec44789a1c365b926ca6c10",
      "b15c6618e9cb425bab1bfad2f4df50b9",
      "10f54afabb3a475f980868d1ec6fe2db",
      "d202f8ee22a64540ae14e0d1aaca5b06",
      "4420088d42ae489bbaf71e923fb560b2",
      "010c26376ec245b8b50b9838094aebb0",
      "61c175f6209b4367a2087749aa8b24e6",
      "8eda8ca0feb34c3cbcd253a759853f54",
      "b4e00e3f960c433e9735a70ee0cb5d1d",
      "0c21d41600ad432aaf56b33aac43beaa",
      "78b1733d5c4042069ef3c79c0a469d94",
      "52a2fe602f3d4985848cd3652ccc7007",
      "9e8add149e5f4ec79b1819d6b772c314",
      "fecb2c3287e94b17bc419263c5febb66",
      "d044330fd5ac4e549ac49d4b2eccde07",
      "d269950be2b741bc89518fb534263ce1",
      "ab4316c3ecea4fea8945508175c65600",
      "4a7dc64d5d8e41d88fc837accbc9fc7d",
      "f9d3842eda984593838e18ed5fac48fe",
      "4abd8e320472464c8737fcac196e658f",
      "273a3f5ad93b4f328df8a4d356498083",
      "7cbb01e5c45f4637b049990bcff4e095",
      "52b82d42eca2457c88798aa58806a912",
      "ff81c231c6a74c0b8bcb5b7fa40734ad",
      "af495b5536fd47d492903dfe33c2c3a3",
      "d42ca0c3bd9f4a9195aedaac4fafb4f8",
      "8035b0fa1255463b85b8d4350fbbda26",
      "5d01b81ea98140b588f888c17dcd7928",
      "1575bffb712a487e9918cadca9b8dc21",
      "436f0feef68b4cccba44945231e2d9ab",
      "37b980eea0d945d59d3b2fb8c191d899",
      "9a99068517c84bcab970f5a44d2c1286",
      "76844b56c80c4a788fc7aef754f31b60",
      "0c66766c4836440f84fa9e33850a0aa2",
      "e61aac3fa1384ec2a63517d0d8b6e7ba",
      "1603d8d35eb248fc9f0c85db4397d2fc",
      "2086cd42bb22465ab7cf74c4ed9c55e7"
     ]
    },
    "id": "55323366",
    "outputId": "50f5bfbf-7862-4951-f832-544a1c4ce217"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading captions from local cache (captions_cache.json)...\n",
      "Captions: 2499\n",
      "Example caption: Small bowel polyps showing malignant cells located at the lamina propria and submucosa. The image is stained with hematoxylin and eosin (H&E) at × 4 magnification.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "CAPTIONS_CACHE = \"captions_cache.json\"\n",
    "\n",
    "if os.path.exists(CAPTIONS_CACHE):\n",
    "    print(f\"Loading captions from local cache ({CAPTIONS_CACHE})...\")\n",
    "    with open(CAPTIONS_CACHE, \"r\") as f:\n",
    "        captions = json.load(f)\n",
    "else:\n",
    "    print(\"Downloading dataset from HuggingFace (first time only)...\")\n",
    "    ds_dict = load_dataset(\"MartiHan/Open-MELON-VL-2.5K\")\n",
    "    ds_all = concatenate_datasets(list(ds_dict.values()))\n",
    "    captions = [str(x) for x in ds_all[\"caption\"]]\n",
    "    with open(CAPTIONS_CACHE, \"w\") as f:\n",
    "        json.dump(captions, f)\n",
    "    print(f\"Captions saved to {CAPTIONS_CACHE}\")\n",
    "\n",
    "print(\"Captions:\", len(captions))\n",
    "print(\"Example caption:\", captions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f1b8e",
   "metadata": {
    "id": "480f1b8e"
   },
   "source": [
    "#### Preprocess the Training Text\n",
    "\n",
    "We join separate captions with `<ENDC>` separator. This helps the model learn boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83044900",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:02:34.787321Z",
     "start_time": "2025-12-27T18:02:34.777342Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83044900",
    "outputId": "109d06f4-1902-4647-96ee-f1e1a6629592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text length (chars): 971654\n",
      "Small bowel polyps showing malignant cells located at the lamina propria and submucosa. The image is stained with hematoxylin and eosin (H&E) at × 4 magnification.\n",
      "<ENDC>\n",
      "Histopathology of small bowel polyps showing malignant cells arranged in diffuse sheets with the presence of intracytoplasmic and extracellular pigments. The image is stained with hematoxylin and eosin (H&E) at × 20 magnification.\n",
      "<ENDC>\n",
      "Small bowel polyps showing malignant cells located at the lamina propria and submucosa. The image is stained with hematoxylin and eosin (H&E) at × 4 magnification.\n",
      "<ENDC>\n",
      "Histopathology showing spindle cell uveal melanoma from a left eye choroidal pigmented malignant melanoma, classified as invasive in the ciliary body and cornea (stage IIIB, pT4bN0M0). The microscopic examination reveals a dense cell proliferation composed of small and medium fusiform (spindle) cells, along with evident pigment production. The tissue fragments were embedded in paraffin for this histopathological exam\n"
     ]
    }
   ],
   "source": [
    "SEP = \"\\n<ENDC>\\n\"\n",
    "text = SEP.join(captions)\n",
    "\n",
    "# Print the total number of characters in the dataset\n",
    "print(\"Training text length (chars):\", len(text))\n",
    "\n",
    "# Print the first 1000 characters of the constructed text corpus\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a82e7",
   "metadata": {
    "id": "f65a82e7"
   },
   "source": [
    "### Character-level Tokenizer\n",
    "\n",
    "We build a vocabulary of unique characters from the training text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0f4cda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:02:42.391469Z",
     "start_time": "2025-12-27T18:02:42.362840Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b0f4cda",
    "outputId": "2114b9c4-85f1-4291-c90b-143ad72cc77c"
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# mapping of characters to numerical tokens (by their order in vocabulary alphabet)\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "\n",
    "# mapping of numerical tokens back to characters\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "def encode(s: str):\n",
    "    \"\"\"\n",
    "    Converts character to a token.\n",
    "    Input: character (e.g. 'A')\n",
    "    Output: numerical token (e.g. 65)\n",
    "    \"\"\"\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ids):\n",
    "    \"\"\"\n",
    "    Converts token to a character.\n",
    "    Input: numerical token (e.g. 65)\n",
    "    Output: character (e.g. 'A')\n",
    "    \"\"\"\n",
    "    return \"\".join(itos[i] for i in ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8e373",
   "metadata": {},
   "source": [
    "Let's print some information about the vocabulary that we have created as well as some examples of encoding words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7547580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 109\n",
      "Preview of the vocabulary: ['\\n', ' ', '\"', '#', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '§', '°', '±', '²', '´', 'µ', '×', 'í', 'α', 'μ', '\\u2005', '\\u2009', '‐', '–', '‘', '’', '“', '”', '→', '⇒', '≤', '≥', '⊷']\n",
      "\n",
      "--- Encoding Examples ---\n",
      "String:  male\n",
      "Tokens:  [71, 59, 70, 63]\n",
      "Mapping: 'm':71, 'a':59, 'l':70, 'e':63\n",
      "----------------------------------------\n",
      "String:  malignant\n",
      "Tokens:  [71, 59, 70, 67, 65, 72, 59, 72, 78]\n",
      "Mapping: 'm':71, 'a':59, 'l':70, 'i':67, 'g':65, 'n':72, 'a':59, 'n':72, 't':78\n",
      "----------------------------------------\n",
      "String:  melanoma\n",
      "Tokens:  [71, 63, 70, 59, 72, 73, 71, 59]\n",
      "Mapping: 'm':71, 'e':63, 'l':70, 'a':59, 'n':72, 'o':73, 'm':71, 'a':59\n",
      "----------------------------------------\n",
      "String:  malignant melanoma\n",
      "Tokens:  [71, 59, 70, 67, 65, 72, 59, 72, 78, 1, 71, 63, 70, 59, 72, 73, 71, 59]\n",
      "Mapping: 'm':71, 'a':59, 'l':70, 'i':67, 'g':65, 'n':72, 'a':59, 'n':72, 't':78, ' ':1, 'm':71, 'e':63, 'l':70, 'a':59, 'n':72, 'o':73, 'm':71, 'a':59\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of the vocabulary:\", vocab_size)\n",
    "print(\"Preview of the vocabulary:\", chars)\n",
    "\n",
    "examples = [\"male\", \"malignant\", \"melanoma\", \"malignant melanoma\"]\n",
    "\n",
    "print(\"\\n--- Encoding Examples ---\")\n",
    "\n",
    "for word in examples:\n",
    "    tokens = encode(word)\n",
    "    \n",
    "    # Create a visual mapping of Char -> Token\n",
    "    mapping_str = \", \".join([f\"'{c}':{t}\" for c, t in zip(word, tokens)])\n",
    "    \n",
    "    print(f\"String:  {word}\")\n",
    "    print(f\"Tokens:  {tokens}\")\n",
    "    print(f\"Mapping: {mapping_str}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa4771",
   "metadata": {},
   "source": [
    "Now we encode the text corpus and store it as PyTorch array. The first 90% of this array will be used for training, while the remaining 10% will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f9a31d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens: 874488 Val tokens: 97166\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split: str):\n",
    "    src = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(src) - cfg.block_size - 1, (cfg.batch_size,))\n",
    "    x = torch.stack([src[i:i+cfg.block_size] for i in ix])\n",
    "    y = torch.stack([src[i+1:i+cfg.block_size+1] for i in ix])\n",
    "    return x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "print(\"Train tokens:\", train_data.numel(), \"Val tokens:\", val_data.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b761c",
   "metadata": {
    "id": "ea3b761c"
   },
   "source": [
    "### Model Training Configurations\n",
    "\n",
    "The following configuration classes define the size of the transformer model and how it is trained. These hyperparameters control model capacity, context length, training stability, and computational cost. You can later use this to perform different experiments, e.g. comparing models of different size (capacity) and context length. \n",
    "\n",
    "**Model configuration**\n",
    "\n",
    "- `vocab_size` - number of unique tokens the model can process. In this assignment, tokens are individual characters, so the vocabulary size equals the number of distinct characters in the training corpus.\n",
    "\n",
    "- `block_size` - the context window length, i.e. the maximum number of tokens the model can see at once. During training and generation, the model predicts the next token using only the previous block_size tokens. A larger block size allows the model to capture longer-range dependencies but increases memory and computation requirements.\n",
    "\n",
    "- `n_layer` - number of stacked transformer decoder blocks. More layers increase model depth and expressiveness but also training time and risk of overfitting.\n",
    "\n",
    "- `n_head` - number of attention heads in each self-attention layer. Multiple heads allow the model to attend to different aspects of the context simultaneously (e.g. syntax, formatting, or local patterns).\n",
    "\n",
    "- `n_embd` - dimensionality of token embeddings and hidden representations. Larger embeddings allow richer representations but increase memory usage and compute cost.\n",
    "\n",
    "- `dropout` - dropout probability used during training as a regularization technique. It helps prevent overfitting by randomly deactivating neurons. Dropout is disabled for CPU training to keep behavior deterministic and training stable.\n",
    "\n",
    "**Training configuration**\n",
    "- `batch_size` - Number of training sequences processed in parallel during one optimization step. Larger batches improve gradient stability but require more memory.\n",
    "\n",
    "- `max_iters` - total number of training iterations (parameter update steps).\n",
    "\n",
    "- `eval_interval` - number of training iterations between evaluations on the validation set.\n",
    "\n",
    "- `eval_iters` - number of mini-batches used to estimate training and validation loss during evaluation.\n",
    "\n",
    "- `lr (learning rate)` - step size used by the optimizer when updating model parameters. This is one of the most sensitive hyperparameters for training stability.\n",
    "\n",
    "- `weight_decay` - regularization term that penalizes large weights and helps reduce overfitting.\n",
    "\n",
    "- `device` - specifies whether training runs on CPU or GPU. Smaller model and batch sizes are used automatically when no GPU is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb2940b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:02:59.466533Z",
     "start_time": "2025-12-27T18:02:59.160335Z"
    },
    "id": "cfb2940b"
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "### Model configuration (number of layers, number of heads, embedding dimensions, dropout) ###\n",
    "##############################################################################################\n",
    "\n",
    "# Configuration for GPU\n",
    "@dataclass\n",
    "class ModelConfigGPU:\n",
    "    vocab_size: int\n",
    "    block_size: int\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 256\n",
    "    dropout: float = 0.2\n",
    "\n",
    "# Configuration for CPU\n",
    "@dataclass\n",
    "class ModelConfigCPU:\n",
    "    vocab_size: int\n",
    "    block_size: int\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0\n",
    "\n",
    "########################################################################################################\n",
    "### Model configuration (block size, batch size, evaluation iterations, learning rate, weight decay) ###\n",
    "########################################################################################################\n",
    "\n",
    "# Configuration for GPU\n",
    "@dataclass\n",
    "class TrainConfigGPU:\n",
    "    block_size: int = 256\n",
    "    batch_size: int = 64\n",
    "    max_iters: int = 2000\n",
    "    eval_interval: int = 250\n",
    "    eval_iters: int = 200\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.1\n",
    "    device: str = \"cuda\"\n",
    "\n",
    "# Configuration for CPU\n",
    "@dataclass\n",
    "class TrainConfigCPU:\n",
    "    block_size: int = 64\n",
    "    batch_size: int = 12\n",
    "    max_iters: int = 2000\n",
    "    eval_interval: int = 200\n",
    "    eval_iters: int = 50\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.1\n",
    "    device: str = \"cpu\"\n",
    "    compile: bool = False\n",
    "\n",
    "\n",
    "# Automatically select suitable configuration based on GPU detection\n",
    "if torch.cuda.is_available():\n",
    "  ModelConfig = ModelConfigGPU\n",
    "  TrainConfig = TrainConfigGPU\n",
    "else:\n",
    "  ModelConfig = ModelConfigCPU\n",
    "  TrainConfig = TrainConfigCPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914a8f7",
   "metadata": {
    "id": "6914a8f7"
   },
   "source": [
    "### Building the NanoGPT Model\n",
    "\n",
    "The model begins with token embeddings, which map character indices to dense vectors, and positional embeddings, which encode the order of tokens within a fixed context window (`block_size`). These embeddings are added together and passed through a stack of transformer decoder blocks.\n",
    "\n",
    "Each decoder block consists of two main submodules: causal self-attention and a feed-forward network (MLP). Causal self-attention allows each token to attend only to previous tokens by applying a causal mask, ensuring that the model cannot access future information during training or generation. Multiple attention heads are used so that the model can focus on different aspects of the context in parallel. The MLP applies a non-linear transformation independently at each position, complementing the attention mechanism. Layer normalization and residual connections are used throughout to stabilize training and preserve information flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "GQjzz6O9zVb2",
   "metadata": {
    "id": "GQjzz6O9zVb2"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, c: ModelConfig):\n",
    "        super().__init__()\n",
    "        assert c.n_embd % c.n_head == 0\n",
    "        self.n_head = c.n_head\n",
    "        self.head_dim = c.n_embd // c.n_head\n",
    "\n",
    "        self.qkv = nn.Linear(c.n_embd, 3 * c.n_embd, bias=False)\n",
    "        self.proj = nn.Linear(c.n_embd, c.n_embd, bias=False)\n",
    "        self.dropout = nn.Dropout(c.dropout)\n",
    "\n",
    "        mask = torch.tril(torch.ones(c.block_size, c.block_size)).view(1, 1, c.block_size, c.block_size)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.proj(y)\n",
    "        y = self.dropout(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, c: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(c.n_embd, 4 * c.n_embd)\n",
    "        self.proj = nn.Linear(4 * c.n_embd, c.n_embd)\n",
    "        self.dropout = nn.Dropout(c.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, c: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(c.n_embd)\n",
    "        self.attn = CausalSelfAttention(c)\n",
    "        self.ln2 = nn.LayerNorm(c.n_embd)\n",
    "        self.mlp = MLP(c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cgvSRhqr1Ltu",
   "metadata": {
    "id": "cgvSRhqr1Ltu"
   },
   "source": [
    "After passing through all decoder blocks, the model applies a final layer normalization and a linear output head that produces a probability distribution over the vocabulary for the next-token prediction. The training objective is standard cross-entropy loss between the predicted next token and the true next token.\n",
    "\n",
    "This NanoGPT model contains approximately between 0.8 and 3 million parameters, depending on the configuration. By comparison, GPT-2 contains hundreds of millions of parameters and modern commercial language models contain many billion parameters. Despite this difference in scale, the same architectural principles apply. The total number of parameters is mainly determined by the embedding dimension (`n_embd`), the number of layers (`n_layer`), and the vocabulary size, while the number of attention heads controls how the embedding space is partitioned rather than its overall size.\n",
    "\n",
    "The model is intentionally kept small so that it can be trained quickly on limited hardware and easily inspected. The goal of this assignment is to understand how transformer components interact, not to achieve state-of-the-art language modeling performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6762045",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:03:04.558560Z",
     "start_time": "2025-12-27T18:03:04.244986Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6762045",
    "outputId": "25cfabc8-6073-4022-b3b1-85b3ce01d555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 0.827392 M\n"
     ]
    }
   ],
   "source": [
    "class NanoGPT(nn.Module):\n",
    "    def __init__(self, c: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.c = c\n",
    "        self.tok_emb = nn.Embedding(c.vocab_size, c.n_embd)\n",
    "        self.pos_emb = nn.Embedding(c.block_size, c.n_embd)\n",
    "        self.drop = nn.Dropout(c.dropout)\n",
    "        self.blocks = nn.ModuleList([Block(c) for _ in range(c.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(c.n_embd)\n",
    "        self.head = nn.Linear(c.n_embd, c.vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.c.block_size\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
    "        x = self.drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "cfg = TrainConfig()\n",
    "mcfg = ModelConfig(vocab_size=vocab_size, block_size=cfg.block_size)\n",
    "\n",
    "model = NanoGPT(mcfg).to(cfg.device)\n",
    "print(\"Parameters:\", sum(p.numel() for p in model.parameters())/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ec49d",
   "metadata": {
    "id": "3a8ec49d"
   },
   "source": [
    "### The Training Loop\n",
    "\n",
    "We use the AdamW optimizer and periodically evaluate on the validation set. The training in Google Colab should take for both the CPU and GPU configurations approximately 4-5 minutes using the baseline configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cf4319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from nanogpt_checkpoint.pt — you can skip the training cell below.\n"
     ]
    }
   ],
   "source": [
    "MODEL_CHECKPOINT = \"nanogpt_checkpoint.pt\"\n",
    "\n",
    "if os.path.exists(MODEL_CHECKPOINT):\n",
    "    model.load_state_dict(torch.load(MODEL_CHECKPOINT, map_location=cfg.device, weights_only=True))\n",
    "    print(f\"Loaded model from {MODEL_CHECKPOINT} — you can skip the training cell below.\")\n",
    "else:\n",
    "    print(f\"No checkpoint found at {MODEL_CHECKPOINT} — run the training cell below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dd3b1d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:04:53.807905Z",
     "start_time": "2025-12-27T18:03:20.624644Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d5f46a7b70a24032bdd01c079189baf4",
      "2aa1f5941b934109ad203c8c2c42dd49",
      "79e8aa1ee73d4befb2a986d99946e8ad",
      "13c646d58aac4b989f228a491bdfde18",
      "71c88adb4d7f48fa860688a28eb3034c",
      "d8088cf79e3f490dbd46664b44c95a92",
      "72b4074f5a1c4010b8978b58bb874f85",
      "0e7826b4890f46e1a75f03737dd998bf",
      "3dc5a25fae164f5897affe5ec41bffc6",
      "785c36956f9a46dca5e5b9fd885de89f",
      "1b3fe486be924068a2bb98eea91dc15c"
     ]
    },
    "id": "4dd3b1d8",
    "outputId": "a37a3ec5-eea6-42eb-e274-c0058a4c93f9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f6f5fe04f14129a549328d72eee057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to nanogpt_checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "MODEL_CHECKPOINT = \"nanogpt_checkpoint.pt\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(cfg.eval_iters)\n",
    "        for k in range(cfg.eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "model.train()\n",
    "pbar = tqdm(range(cfg.max_iters), desc=\"training\")\n",
    "for it in pbar:\n",
    "    if it % cfg.eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        pbar.set_postfix(train=losses[\"train\"], val=losses[\"val\"])\n",
    "\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    _, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Save trained model to disk so it can be reloaded without retraining\n",
    "torch.save(model.state_dict(), MODEL_CHECKPOINT)\n",
    "print(f\"Model saved to {MODEL_CHECKPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6af6d0",
   "metadata": {
    "id": "3b6af6d0"
   },
   "source": [
    "### Text Generation (Sampling)\n",
    "\n",
    "Now that the model is trained we can put it to use. We generate the synthetic captions starting from `\"H&E stained section showing\"` string by autoregressively sampling next characters\n",
    "\n",
    "Hyperparameters:\n",
    "- `temperature`: higher = more random, lower = less variety\n",
    "- `top_k`: restrict sampling to top-k most likely chars (lower number eliminates less likely candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7856711",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7856711",
    "outputId": "13c1a5c3-7c66-4839-dcea-4e5a0ca97ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H&E stained section showing malignant melanophages in the contributed tumor sebaces, and desping tumoral cell, charact cells are pagints of the margins. Stain ditic cyst, observed tumor and are pleomorphism, is no an o confined to these diagnosis of the right super margin fibrosis displaying in an an angiogic taken one subcutaneous (SCC).\n",
      "<ENDC>\n",
      "Histopathology image is an intradermal neoplasm (IT) in an eosing sometive irregular nucleated to the lesion is and epidermis with areated extension the porly diagnosed with as co\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(start: str, max_new_tokens=400, temperature=1.0, top_k=60):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([encode(start)], dtype=torch.long, device=cfg.device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -cfg.block_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, k=min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "prompt = \"H&E stained section showing\"\n",
    "\n",
    "print(generate(prompt, max_new_tokens=500, temperature=0.7, top_k=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8m0HEEYvdI9n",
   "metadata": {
    "id": "8m0HEEYvdI9n"
   },
   "source": [
    "⚠️ *Everything below this line must be submitted as a deliverable for this assignment.*\n",
    "\n",
    "ℹ️ *Answering the exercises below will require you to implement new code and/or modify the code in cells above. You can add the code directly in the notebook or in separate Python files, depending on your preference. If you write code in separate files, please do not forget to also submit them. The exercises will also require re-running the training, sometimes multiple times. You can split the workload among the group members so things are done more efficiently.*\n",
    "\n",
    "## Exercises: Practice\n",
    "\n",
    "#### Exercise 1 \n",
    "\n",
    "When we created the text corpus, we used the `<ENDC>` separator to mark the end of a caption. Why is the `<ENDC>` separator needed? What would happen if you use a model trained without such separator in practice? In order to investigate this, train a model without the `<ENDC>` separator and compare the results with the model trained with the `<ENDC>` separator when generating captions.\n",
    "\n",
    "Output with the `<ENDC>` separator (```SEP = \"\\n<ENDC>\\n\"```):\n",
    "```\n",
    "H&E stained section showing a displays and an and solites the stromal perimary incouse and in subtased and plominant this stained with Hematoxylin and Eosin (H&E) (H&E) stain, 100X).\n",
    "<ENDC>\n",
    "Histopathology of atypical magnification of ocular small, shower can man for significant melanocytes, stained with Hematoxylin and Eosin (H&E) stainification ×100)\n",
    "<ENDC>\n",
    "Histopathological showing and a proliferating of a matures. The image shows epithelial connece of the cellulcination of and considures. The image displays considuct n\n",
    "```\n",
    "Output with \"\\n\" as seperator (```SEP = \"\\n\"```):\n",
    "```\n",
    "H&E stained section showing a columnar melanoma. The image is stained with Hematoxylin and Eosin (HE) at 20× magnigination, original magnification ×40.\n",
    "Histopathology of a composed of malignant melanoma. Stained with Hematoxylin and Eosin (H&E).\n",
    "Histopathology of a lentigo malignant melanoma of adulte stroma. These lesion was depictween dege of melanoma (SSM) from a subsequent inten area the necrosis (indicated by the brears of the lesion indeterming a nested slight follihin patients an and combedent, a pleomorphism conta\n",
    "```\n",
    "\n",
    "Output with \"\" as seperator (```SEP = \"\"```):\n",
    "```\n",
    "H&E stained section showing a shapeact of a sollagen with in the diagnosis of the dermal presenting intraepitheling siver present with intradermal lesion from a presenting at marganul case and as of phistior and nodullation, characterized by andell-shared arrow cells and are tumor cells arrows. This component in a and arrow an extendings of a celles and and sellow his arring in the dermal epithelioid and sugfivastent with a malignant nevolying a pigmentation of shiver biopsy from a presenting at malignant melanoma showing\n",
    "```\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Train the model and plot training loss and validation loss as a function of training iterations. Modify the following hyperparameters and observe the effect:\n",
    "- Learning rate\n",
    "- Batch size\n",
    "- Evaluation interval\n",
    "\n",
    "Explain why the observed changes occur. What patterns indicate underfitting? What patterns suggest overfitting or unstable training?\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "We can reduce the number of tokens by converting all letters to either uppercase or lowercase. Implement this in the preprocessing function and retrain a model. Report your observations on the training and performance of the model.\n",
    "\n",
    "\n",
    "#### Exercise 4\n",
    "\n",
    "The dataset contains 2500 figure captions. After encoding and splitting out 10% of the tokens for validation, we are left with 874488 training tokens. Is this also the number of training samples used to train our NanoGPT model? If yes, explain why, if not try to estimate the actual number of training samples.\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "Text generation depends strongly on the sampling hyperparameters.\n",
    "\n",
    "Generate captions using at least three different temperature values and two different top_k values. Include at least 10 generated examples per hyperparameter configuration in your report.\n",
    "\n",
    "For each configuration, comment on the following characteristics of the generated samples:\n",
    "  - Fluency and structure\n",
    "  - Repetition or degeneration\n",
    "  - Factual plausibility (even if the content is synthetic)\n",
    "\n",
    "Identify optimal configuration of parameters that balances coherence and diversity of the produced synthetic captions, and justify your choice.\n",
    "\n",
    "## Flipped Classroom Log\n",
    "\n",
    "ℹ️ *You have to fill this log for both flipped classroom sessions for this assignment. You only fill the log for your group, not together with the group that you interacted with.*\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Preparation\n",
    "* **Clients:** List specific theoretical or implementation (code) questions prepared before class.\n",
    "* **Consultants:** List papers, videos, or code documentation reviewed to prepare. Note, this is not limited to the material listed above, you can add any new material that you used or found useful. \n",
    "\n",
    "#### Peer Interaction\n",
    "* **Clients:** Summarize the solutions or explanations received.\n",
    "* **Consultants:** Summarize advice given and specific resources shared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0927484",
   "metadata": {},
   "source": [
    "### Exercise 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "15cc3939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EX2_LOG_FILE = \"ex2_experiments.json\"\n",
    "\n",
    "def load_ex2_log():\n",
    "    \"\"\"Load existing experiment log or return empty list.\"\"\"\n",
    "    if os.path.exists(EX2_LOG_FILE):\n",
    "        with open(EX2_LOG_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return []\n",
    "\n",
    "def save_ex2_log(log):\n",
    "    \"\"\"Save experiment log to disk.\"\"\"\n",
    "    with open(EX2_LOG_FILE, \"w\") as f:\n",
    "        json.dump(log, f, indent=2)\n",
    "\n",
    "#utilities for our own use, not for grading\n",
    "def list_experiments():\n",
    "    log = load_ex2_log()\n",
    "    for i, exp in enumerate(log):\n",
    "        hp = exp[\"hyperparameters\"]\n",
    "        print(f\"  [{i}] {exp['label']:<25} train={exp['final_train_loss']:.4f}  val={exp['final_val_loss']:.4f}\")\n",
    "\n",
    "def delete_experiment(index):\n",
    "    log = load_ex2_log()\n",
    "    removed = log.pop(index)\n",
    "    save_ex2_log(log)\n",
    "    print(f\"Deleted: '{removed['label']}'. Remaining: {len(log)} experiments.\")\n",
    "\n",
    "def clear_all_experiments():\n",
    "    save_ex2_log([])\n",
    "    print(\"All experiments cleared.\")\n",
    "\n",
    "\n",
    "def run_ex2_experiment(lr=1e-3, batch_size=12, eval_interval=200, \n",
    "                       max_iters=2000, eval_iters=50, label=None):\n",
    "    \"\"\"\n",
    "    Train a fresh model with the given hyperparameters.\n",
    "    Logs train/val losses at each eval step and saves to ex2_experiments.json.\n",
    "    \n",
    "    Args:\n",
    "        lr: learning rate\n",
    "        batch_size: batch size  \n",
    "        eval_interval: how often to evaluate\n",
    "        max_iters: total training iterations\n",
    "        eval_iters: batches used per evaluation\n",
    "        label: optional label for this run\n",
    "    \n",
    "    Returns:\n",
    "        dict with experiment results\n",
    "    \"\"\"\n",
    "    if label is None:\n",
    "        parts = []\n",
    "        if lr != 1e-3: parts.append(f\"lr={lr}\")\n",
    "        if batch_size != 12: parts.append(f\"bs={batch_size}\")\n",
    "        if eval_interval != 200: parts.append(f\"eval_int={eval_interval}\")\n",
    "        label = \", \".join(parts) if parts else \"baseline\"\n",
    "    \n",
    "    cfg.lr = lr\n",
    "    cfg.batch_size = batch_size\n",
    "    cfg.eval_interval = eval_interval\n",
    "    cfg.max_iters = max_iters\n",
    "    cfg.eval_iters = eval_iters\n",
    "    \n",
    "    fresh_model = NanoGPT(mcfg).to(cfg.device)\n",
    "    opt = torch.optim.AdamW(fresh_model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    \n",
    "    eval_steps = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _estimate_loss():\n",
    "        fresh_model.eval()\n",
    "        out = {}\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            losses_t = torch.zeros(cfg.eval_iters)\n",
    "            for k in range(cfg.eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                _, l = fresh_model(X, Y)\n",
    "                losses_t[k] = l.item()\n",
    "            out[split] = losses_t.mean().item()\n",
    "        fresh_model.train()\n",
    "        return out\n",
    "    \n",
    "    fresh_model.train()\n",
    "    pbar = tqdm(range(cfg.max_iters), desc=f\"Ex2: {label}\")\n",
    "    for it in pbar:\n",
    "        if it % cfg.eval_interval == 0:\n",
    "            losses = _estimate_loss()\n",
    "            eval_steps.append(it)\n",
    "            train_losses.append(losses[\"train\"])\n",
    "            val_losses.append(losses[\"val\"])\n",
    "            pbar.set_postfix(train=f\"{losses['train']:.4f}\", val=f\"{losses['val']:.4f}\")\n",
    "        \n",
    "        xb, yb = get_batch(\"train\")\n",
    "        _, loss = fresh_model(xb, yb)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    final = _estimate_loss()\n",
    "    eval_steps.append(cfg.max_iters)\n",
    "    train_losses.append(final[\"train\"])\n",
    "    val_losses.append(final[\"val\"])\n",
    "    \n",
    "    result = {\n",
    "        \"label\": label,\n",
    "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "        \"hyperparameters\": {\n",
    "            \"lr\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"eval_interval\": eval_interval,\n",
    "            \"max_iters\": max_iters,\n",
    "            \"eval_iters\": eval_iters,\n",
    "        },\n",
    "        \"eval_steps\": eval_steps,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"final_train_loss\": final[\"train\"],\n",
    "        \"final_val_loss\": final[\"val\"],\n",
    "    }\n",
    "    \n",
    "    log = load_ex2_log()\n",
    "    log.append(result)\n",
    "    save_ex2_log(log)\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def draw_plots(log_file=\"\"):\n",
    "    if log_file:\n",
    "        with open(log_file, \"r\") as f:\n",
    "            log = json.load(f)\n",
    "    else:\n",
    "        log = load_ex2_log()\n",
    "    if not log:\n",
    "        print(\"No experiments logged yet. Run some experiments first!\")\n",
    "    else:\n",
    "        print(f\"{'Label':<25} {'LR':<10} {'BS':<6} {'Eval Int':<10} {'Train Loss':<12} {'Val Loss':<12}\")\n",
    "        for exp in log:\n",
    "            hp = exp[\"hyperparameters\"]\n",
    "            print(f\"{exp['label']:<25} {hp['lr']:<10} {hp['batch_size']:<6} \"\n",
    "                f\"{hp['eval_interval']:<10} {exp['final_train_loss']:<12.4f} {exp['final_val_loss']:<12.4f}\")\n",
    "        \n",
    "        def categorize(exp):\n",
    "            hp = exp[\"hyperparameters\"]\n",
    "            cats = []\n",
    "            if hp[\"lr\"] != 1e-3 or exp[\"label\"] == \"baseline\": cats.append(\"learning_rate\")\n",
    "            if hp[\"batch_size\"] != 12 or exp[\"label\"] == \"baseline\": cats.append(\"batch_size\") \n",
    "            if hp[\"eval_interval\"] != 200 or exp[\"label\"] == \"baseline\": cats.append(\"eval_interval\")\n",
    "            if not cats: cats = [\"other\"]\n",
    "            return cats\n",
    "        \n",
    "        groups = {\"learning_rate\": [], \"batch_size\": [], \"eval_interval\": [], \"other\": []}\n",
    "        for exp in log:\n",
    "            for cat in categorize(exp):\n",
    "                if cat in groups:\n",
    "                    groups[cat].append(exp)\n",
    "        \n",
    "        groups = {k: v for k, v in groups.items() if v}\n",
    "        \n",
    "        fig, axes = plt.subplots(1, len(groups), figsize=(7 * len(groups), 5), squeeze=False)\n",
    "        \n",
    "        colors = plt.cm.tab10.colors\n",
    "        group_titles = {\n",
    "            \"learning_rate\": \"Effect of Learning Rate\",\n",
    "            \"batch_size\": \"Effect of Batch Size\",\n",
    "            \"eval_interval\": \"Effect of Evaluation Interval\",\n",
    "        }\n",
    "        \n",
    "        for ax_idx, (group_name, exps) in enumerate(groups.items()):\n",
    "            ax = axes[0][ax_idx]\n",
    "            for i, exp in enumerate(exps):\n",
    "                c = colors[i % len(colors)]\n",
    "                ax.plot(exp[\"eval_steps\"], exp[\"train_losses\"], \n",
    "                        color=c, linestyle=\"-\", label=f'{exp[\"label\"]} (train)')\n",
    "                ax.plot(exp[\"eval_steps\"], exp[\"val_losses\"], \n",
    "                        color=c, linestyle=\"--\", label=f'{exp[\"label\"]} (val)')\n",
    "            ax.set_xlabel(\"Training Iteration\")\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "            ax.set_title(group_titles.get(group_name, group_name))\n",
    "            ax.legend(fontsize=\"small\")\n",
    "        plt.tight_layout()\n",
    "        save_name = os.path.splitext(log_file)[0] + \".png\" if log_file else \"ex2_loss_plots.png\"\n",
    "        plt.savefig(save_name, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        print(f\"Plot saved to {save_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2abba95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#two blocks of experiment calls for demonstrating average changes and extreme changes\n",
    "\n",
    "def reproduce_average_changes():\n",
    "    clear_all_experiments()\n",
    "    run_ex2_experiment() #1. baseline\n",
    "    #average changes:\n",
    "    run_ex2_experiment(lr=1e-4, label=\"lr=1e-4 (low)\") # 2. Low learning rate\n",
    "    run_ex2_experiment(lr=1e-2, label=\"lr=1e-2 (high)\") # 3. High learning rate\n",
    "    run_ex2_experiment(batch_size=4, label=\"bs=4 (small)\") # 4. Small batch size\n",
    "    run_ex2_experiment(batch_size=32, label=\"bs=32 (large)\") # 5. Large batch size\n",
    "    run_ex2_experiment(eval_interval=50, label=\"eval_int=50 (freq)\") # 6. Frequent eval\n",
    "    run_ex2_experiment(eval_interval=500, label=\"eval_int=500 (rare)\") # 7. Rare eval\n",
    "    draw_plots()\n",
    "\n",
    "def reproduce_extreme_changes():\n",
    "    clear_all_experiments()\n",
    "    run_ex2_experiment()\n",
    "    #extreme changes:                                            \n",
    "    run_ex2_experiment(lr=1e-5, label=\"lr=1e-5 (very low)\") # 2. Very low learning rate\n",
    "    run_ex2_experiment(lr=5e-2, label=\"lr=5e-2 (very high)\") # 3. Very high learning rate\n",
    "    run_ex2_experiment(batch_size=2, label=\"bs=2 (tiny)\") # 4. Tiny batch size\n",
    "    run_ex2_experiment(batch_size=64, label=\"bs=64 (very large)\") # 5. Very large batch size\n",
    "    run_ex2_experiment(eval_interval=25, label=\"eval_int=25 (freq)\") # 6. Very frequent eval\n",
    "    run_ex2_experiment(eval_interval=1000, label=\"eval_int=1000 (rare)\") # 7. Very rare eval\n",
    "    draw_plots()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a3e15",
   "metadata": {},
   "source": [
    "### Exercise 2 Analysis\n",
    "\n",
    "We ran 12 experiments: a baseline plus 6 variations with moderate (\"average\") changes, and a baseline plus 6 variations with extreme changes. All experiments used `max_iters=2000` on CPU with `block_size=64`. To reproduce the experiments, run `reproduce_average_changes()` and `reproduce_extreme_changes()`.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebbafa6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Effect of Learning Rate\n",
    "\n",
    "- Low learning rate (1e-4): The optimizer takes very small steps, so the model learns slowly. After 2000 iterations the loss is still high (val=1.46 vs. baseline 1.12), indicating the model has not yet converged. Given more iterations, it would likely eventually reach a similar loss, but within this fixed budget it appears undertrained. This is not underfitting in the classical sense, the model has sufficient capacity, it simply hasn't had enough effective optimization steps. To verify this, we ran both the baseline (`lr=1e-3`) and the low learning rate (`lr=1e-4`) for 6000 iterations (3 times the default budget). With 6000 iterations, `lr=1e-4` reached train=0.9916 and val=1.0926, lower than the baseline at 2000 iterations (train~1.05, val~1.12), confirming that the model was indeed just undertrained and continued to improve given more time. However, the baseline `lr=1e-3` at 6000 iterations reached train=0.8085 and val=0.9532, which is substantially lower still. This shows that even with 3x more iterations, the lower learning rate cannot catch up to the default, and the optimizer's small step size remains a bottleneck, leaving the model underfitted relative to what the same architecture can achieve with a better tuned learning rate.\n",
    "\n",
    "<a id=\"fig-lr-6000\"></a>\n",
    "\n",
    "![Figure 1: Effect of Learning Rate, 6000 iterations](6000iters.png)\n",
    "\n",
    "*Figure 1: Loss curves for baseline (lr=1e-3) vs. low learning rate (lr=1e-4) over 6000 iterations.*\n",
    "\n",
    "The loss curves for the average learning rate experiments are shown in [Figure 2](#fig-lr-average). The extreme learning rate experiments are shown in [Figure 3](#fig-lr-extreme).\n",
    "\n",
    "<a id=\"fig-lr-average\"></a>\n",
    "\n",
    "![Figure 2: Effect of Learning Rate, average experiments](task1deliverables/ex2_experiments-average_learning_rate.png)\n",
    "\n",
    "*Figure 2: Loss curves for moderate learning rate variations (lr=1e-4, baseline 1e-3, lr=1e-2).*\n",
    "\n",
    "<a id=\"fig-lr-extreme\"></a>\n",
    "\n",
    "![Figure 3: Effect of Learning Rate, extreme experiments](task1deliverables/ex2_experiments-extreme_learning_rate.png)\n",
    "\n",
    "*Figure 3: Loss curves for extreme learning rate variations (lr=1e-5, baseline 1e-3, lr=5e-2).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ddb00",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- High learning rate (1e-2): The optimizer takes overly large steps, causing it to overshoot minima. The loss curve shows instability because the loss occasionally rises before descending again (e.g. from 2.30 at iter 1400 back up to 2.33 at iter 1600), as visible in [Figure 2](#fig-lr-average). The final loss (val=2.04) is worse than even the low learning rate, because the large steps prevent the optimizer from settling into a good region of the loss landscape. This is optimization instability, not underfitting.\n",
    "\n",
    "The extreme parameter experiments confirm these patterns more dramatically (see [Figure 3](#fig-lr-extreme)):\n",
    "- lr=1e-5: Severe undertraining, the loss barely decreases from its initial value (val=2.71), because the tiny step size means the model has made almost no meaningful progress in 2000 iterations.\n",
    "- lr=5e-2: Clear divergence. The loss oscillates heavily throughout training (e.g. dropping to 2.71 at iter 1600, then rising back up to 3.04 at iter 2000). The optimizer never converges, it repeatedly overshoots the minima and the loss actually increases toward the end.\n",
    "\n",
    "Conclusion: \n",
    "The baseline learning rate of 1e-3 is well tuned for this model and dataset. Too low learning rates cause slow convergence (the model can't make enough progress within the iteration budget), while too high learning rates cause optimization instability or divergence (the optimizer overshoots and oscillates). The optimal learning rate depends on the specific interaction between the optimizer (AdamW maintains moving averages of gradients), the loss landscape shape, and the gradient magnitudes so it cannot be derived analytically and must be found empirically.\n",
    "\n",
    "\n",
    "#### Effect of Batch Size\n",
    "\n",
    "Larger batch sizes led to lower final losses, while smaller batch sizes led to higher losses (underfitting). The average batch size experiments are shown in [Figure 4](#fig-bs-average), and the extreme experiments in [Figure 5](#fig-bs-extreme).\n",
    "\n",
    "- bs=32: Final val loss = 0.97, significantly better than the baseline (1.12).\n",
    "- bs=4: Final val loss = 1.48, significantly worse.\n",
    "\n",
    "This effect is primarily explained by how much data the model sees in a fixed number of iterations. Each iteration, `get_batch` constructs a tensor of shape `(batch_size, block_size)`, that is, `batch_size` sequences of `block_size` consecutive tokens each. The `block_size` (64 on CPU) is the model's context window: the maximum number of tokens the model can see at once when predicting the next token. So each iteration processes `batch_size x block_size` tokens in total.\n",
    "\n",
    "With a fixed budget of `max_iters = 2000`:\n",
    "\n",
    "$$\\text{Tokens per iteration} = \\text{batch size} \\times \\text{block size}$$\n",
    "$$\\text{Total tokens seen} = \\text{batch size} \\times \\text{block size} \\times \\text{max iters}$$\n",
    "\n",
    "The full encoded corpus has ~971,654 characters (= tokens, since we use character level tokenization). After the 90/10 train/val split: `train_tokens = floor(0.9 x 971,654) = 874,488`. One epoch means the model has seen this many tokens in total.\n",
    "\n",
    "$$\\text{Estimated epochs} = \\frac{\\text{batch size} \\times \\text{block size} \\times \\text{max iters}}{\\text{train tokens}} = \\frac{\\text{batch size} \\times 64 \\times 2000}{874{,}488}$$\n",
    "\n",
    "Worked example for the baseline (`bs=12`):\n",
    "- Tokens/iteration: 12 x 64 = 768\n",
    "- Total tokens seen: 768 x 2000 = 1,536,000\n",
    "- Estimated epochs: 1,536,000 / 874,488 = 1.76\n",
    "\n",
    "With `bs=32`, the model effectively trains for ~4.7 epochs vs. ~1.8 for the baseline. It sees the dataset roughly 2.7 times more, which could explain the better performance. Additionally, larger batches produce more stable gradient estimates (less noise per update), which helps the AdamW optimizer make more consistent progress.\n",
    "\n",
    "With `bs=4`, the model sees less than one full epoch of data, meaning it has not even seen every training example once. The noisy small batch gradients also make optimization less efficient.\n",
    "\n",
    "The extreme experiments reinforce this (see [Figure 5](#fig-bs-extreme)):\n",
    "- bs=64: Best result overall (val=0.93), ~9.4 epochs of data exposure. However, we also observe a growing gap between train loss (0.75) and val loss (0.93), which is also a sign of overfitting.\n",
    "- bs=2: Worst batch size result (val=1.86), with the model seeing only ~29% of the training data.\n",
    "\n",
    "<a id=\"fig-bs-average\"></a>\n",
    "\n",
    "![Figure 4: Effect of Batch Size, average experiments](task1deliverables/ex2_experiments-average_batch_size.png)\n",
    "\n",
    "*Figure 4: Loss curves for moderate batch size variations (bs=4, baseline 12, bs=32).*\n",
    "\n",
    "<a id=\"fig-bs-extreme\"></a>\n",
    "\n",
    "![Figure 5: Effect of Batch Size, extreme experiments](task1deliverables/ex2_experiments-extreme_batch_size.png)\n",
    "\n",
    "*Figure 5: Loss curves for extreme batch size variations (bs=2, baseline 12, bs=64).*\n",
    "\n",
    "Conclusion: Within a fixed iteration budget, larger batch sizes are beneficial because the model sees more data. However, very large batch sizes can lead to overfitting, as seen in the widening gap for bs=64.\n",
    "\n",
    "\n",
    "\n",
    "#### Effect of Evaluation Interval\n",
    "\n",
    "The evaluation interval does not affect training, it only changes how frequently we pause to estimate the loss. The model's parameter updates are identical regardless of how often we evaluate. This is confirmed by the nearly identical final losses across all evaluation intervals:\n",
    "\n",
    "| Eval Interval | Final Train Loss | Final Val Loss |\n",
    "|:---|:---|:---|\n",
    "| 25 (very freq) | 1.028 | 1.141 |\n",
    "| 50 (freq) | 1.057 | 1.185 |\n",
    "| 200 (baseline) | 1.053 | 1.116 |\n",
    "| 500 (rare) | 1.054 | 1.166 |\n",
    "| 1000 (very rare) | 1.075 | 1.143 |\n",
    "\n",
    "The small differences between runs are due to random initialization, each experiment trains a fresh model with different random weights, so no two runs are exactly identical even with the same hyperparameters.\n",
    "\n",
    "The \"training loss\" plotted is not the batch loss at each step. It is an estimate computed by `_estimate_loss()`, which averages over 50 random batches at each evaluation point. With `eval_interval=25`, we compute this estimate 80 times during training, producing a densely sampled (and noisier looking) curve. With `eval_interval=1000`, we compute it only twice (plus the final evaluation), producing a straight line between just 3 points. The underlying training trajectory is the same and we observe it at different resolutions, as shown in [Figure 6](#fig-eval-average) and [Figure 7](#fig-eval-extreme).\n",
    "\n",
    "<a id=\"fig-eval-average\"></a>\n",
    "\n",
    "![Figure 6: Effect of Evaluation Interval, average experiments](task1deliverables/ex2_experiments-average_eval_interval.png)\n",
    "\n",
    "*Figure 6: Loss curves for moderate eval interval variations (25, 50, baseline 200, 500).*\n",
    "\n",
    "<a id=\"fig-eval-extreme\"></a>\n",
    "\n",
    "![Figure 7: Effect of Evaluation Interval, extreme experiments](task1deliverables/ex2_experiments-extreme_eval_interval.png)\n",
    "\n",
    "*Figure 7: Loss curves for extreme eval interval variations (25, baseline 200, 1000).*\n",
    "\n",
    "While the eval interval does not affect the model's final quality, it does affect total training time. Each evaluation call runs `_estimate_loss()`, which performs `2 x eval_iters = 100` extra forward passes (50 per split) with `@torch.no_grad()`. These passes do not update the model but still take time. With `max_iters=2000`, the overhead scales directly with the number of evaluations.\n",
    "\n",
    "Setting `eval_interval=25` produces 40x more evaluation overhead than `eval_interval=1000`. On CPU, each `_estimate_loss()` call is nontrivial, so very frequent evaluation noticeably slows down the total run.\n",
    "\n",
    "Conclusion: More frequent evaluation does not improve the model. Less frequent evaluation trains faster, but provides less visibility into the learning dynamics (loss curves). The figures confirm that differing eval intervals produce identical training trajectories sampled at different resolutions.\n",
    "\n",
    "\n",
    "#### Overfitting vs. Underfitting Patterns\n",
    "\n",
    "- Underfitting is indicated by high train and val loss, the model has not learned enough. This was observed with low learning rates ([Figure 2](#fig-lr-average), [Figure 3](#fig-lr-extreme)) and small batch sizes ([Figure 4](#fig-bs-average), [Figure 5](#fig-bs-extreme)).\n",
    "- Overfitting is indicated by a large gap between low train loss and higher val loss when the model memorizes training data. Early signs of this were visible with bs=64 (train=0.75, val=0.93, gap=0.18) compared to the baseline (train=1.05, val=1.12, gap=0.07), as shown in [Figure 5](#fig-bs-extreme).\n",
    "- Unstable training is indicated by nonmonotonic loss curves (loss climbing back up). This was observed with high learning rates (lr=1e-2 and especially lr=5e-2), visible in [Figure 2](#fig-lr-average) and [Figure 3](#fig-lr-extreme).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4311374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14bef2e5",
   "metadata": {},
   "source": [
    "### Logs\n",
    "#### First Flipped Classroom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b95de",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ℹ️ *Write the log for your group in this cell. Should be in a narrative style, aim for a max. of 600 words.*\n",
    "\n",
    "**Role**: Client\n",
    "\n",
    "**Description** of activities during flipped classroom:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa3daf",
   "metadata": {},
   "source": [
    "#### Second Flipped Classroom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f175ca",
   "metadata": {},
   "source": [
    "ℹ️ *Write the log for your group in this cell. Should be in a narrative style, aim for a max. of 600 words.*\n",
    "\n",
    "**Role**: Client/Consultant\n",
    "\n",
    "**Description** of activities during flipped classroom:"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
